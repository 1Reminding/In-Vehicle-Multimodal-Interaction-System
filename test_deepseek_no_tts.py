#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSeek APIæµ‹è¯•è„šæœ¬ - æ— TTSç‰ˆæœ¬

æµ‹è¯•å¤šæ¨¡æ€æ•°æ®èåˆåŠŸèƒ½ï¼Œä¸ä½¿ç”¨è¯­éŸ³åé¦ˆ
"""

import time
from modules.ai.deepseek_client import deepseek_client, MultimodalInput


def test_distraction_complete_scenario():
    """æµ‹è¯•å®Œæ•´çš„åˆ†å¿ƒé©¾é©¶åœºæ™¯ï¼šæ£€æµ‹â†’ç¡®è®¤â†’æ¢å¤"""
    print("ğŸš¨ æµ‹è¯•åœºæ™¯: å®Œæ•´åˆ†å¿ƒé©¾é©¶æµç¨‹")
    print("=" * 50)
    
    # ç¬¬ä¸€æ­¥ï¼šåˆ†å¿ƒæ£€æµ‹
    print("ğŸ“ æ­¥éª¤1: åˆ†å¿ƒé©¾é©¶æ£€æµ‹")
    multimodal_input_1 = MultimodalInput(
        gaze_data={
            "state": "right",
            "duration": 4.5,
            "deviation": "severe"
        },
        gesture_data={
            "gesture": "none",
            "confidence": 0.0,
            "intent": "unknown"
        },
        speech_data={
            "text": "",
            "intent": "unknown",
            "emotion": "neutral"
        },
        timestamp=time.time(),
        duration=4.5
    )
    
    print("ğŸ“Š è¾“å…¥æ•°æ®:")
    print(f"   ğŸ‘ çœ¼åŠ¨: {multimodal_input_1.gaze_data['state']} ({multimodal_input_1.gaze_data['duration']:.1f}s)")
    print(f"   ğŸ– æ‰‹åŠ¿: {multimodal_input_1.gesture_data['gesture']}")
    print(f"   ğŸ¤ è¯­éŸ³: '{multimodal_input_1.speech_data['text']}'")
    
    print("\nğŸ¤– æ­£åœ¨è°ƒç”¨DeepSeek API...")
    ai_response_1 = deepseek_client.analyze_multimodal_data(multimodal_input_1)
    
    print(f"\nğŸ§  AIåˆ†æç»“æœ:")
    print(f"   ğŸ“‹ æ¨èæ“ä½œ: {ai_response_1.recommendation_text}")
    print(f"   ğŸ¯ ç½®ä¿¡åº¦: {ai_response_1.confidence:.2f}")
    print(f"   ğŸ’­ æ¨ç†è¿‡ç¨‹: {ai_response_1.reasoning}")
    print(f"   âš™ï¸ æ“ä½œæŒ‡ä»¤: {ai_response_1.action_code}")
    
    # æ¨¡æ‹Ÿç”¨æˆ·ååº”æ—¶é—´
    print("\nâ³ ç­‰å¾…ç”¨æˆ·å“åº”...")
    time.sleep(2)
    
    # ç¬¬äºŒæ­¥ï¼šç”¨æˆ·è¯­éŸ³ç¡®è®¤
    print("\nğŸ“ æ­¥éª¤2: ç”¨æˆ·è¯­éŸ³ç¡®è®¤")
    multimodal_input_2 = MultimodalInput(
        gaze_data={
            "state": "center",  # ç”¨æˆ·å·²å°†è§†çº¿è½¬å›
            "duration": 1.0,
            "deviation": "normal"
        },
        gesture_data={
            "gesture": "thumbs_up",  # ç«–æ‹‡æŒ‡ç¡®è®¤
            "confidence": 0.88,
            "intent": "confirm"
        },
        speech_data={
            "text": "å·²æ³¨æ„é“è·¯",  # è¯­éŸ³ç¡®è®¤
            "intent": "confirm",
            "emotion": "positive"
        },
        timestamp=time.time(),
        duration=2.0
    )
    
    print("ğŸ“Š è¾“å…¥æ•°æ®:")
    print(f"   ğŸ‘ çœ¼åŠ¨: {multimodal_input_2.gaze_data['state']} ({multimodal_input_2.gaze_data['duration']:.1f}s)")
    print(f"   ğŸ– æ‰‹åŠ¿: {multimodal_input_2.gesture_data['gesture']} (ç½®ä¿¡åº¦: {multimodal_input_2.gesture_data['confidence']:.2f})")
    print(f"   ğŸ¤ è¯­éŸ³: '{multimodal_input_2.speech_data['text']}'")
    
    print("\nğŸ¤– æ­£åœ¨è°ƒç”¨DeepSeek API...")
    ai_response_2 = deepseek_client.analyze_multimodal_data(multimodal_input_2)
    
    print(f"\nğŸ§  AIåˆ†æç»“æœ:")
    print(f"   ğŸ“‹ æ¨èæ“ä½œ: {ai_response_2.recommendation_text}")
    print(f"   ğŸ¯ ç½®ä¿¡åº¦: {ai_response_2.confidence:.2f}")
    print(f"   ğŸ’­ æ¨ç†è¿‡ç¨‹: {ai_response_2.reasoning}")
    print(f"   âš™ï¸ æ“ä½œæŒ‡ä»¤: {ai_response_2.action_code}")


def test_distraction_gesture_only():
    """æµ‹è¯•ä»…æ‰‹åŠ¿ç¡®è®¤çš„åˆ†å¿ƒæ¢å¤"""
    print("\n" + "="*50)
    print("ğŸ– æµ‹è¯•åœºæ™¯: æ‰‹åŠ¿ç¡®è®¤æ¢å¤")
    
    # åˆ†å¿ƒçŠ¶æ€ + æ‰‹åŠ¿ç¡®è®¤ï¼ˆæ— è¯­éŸ³ï¼‰
    multimodal_input = MultimodalInput(
        gaze_data={
            "state": "center",  # å·²å›åˆ°ä¸­å¿ƒ
            "duration": 0.8,
            "deviation": "normal"
        },
        gesture_data={
            "gesture": "peace",  # Væ‰‹åŠ¿è¡¨ç¤ºOK
            "confidence": 0.92,
            "intent": "ok"
        },
        speech_data={
            "text": "",  # æ— è¯­éŸ³è¾“å…¥
            "intent": "unknown",
            "emotion": "neutral"
        },
        timestamp=time.time(),
        duration=1.5
    )
    
    print("ğŸ“Š è¾“å…¥æ•°æ®:")
    print(f"   ğŸ‘ çœ¼åŠ¨: {multimodal_input.gaze_data['state']}")
    print(f"   ğŸ– æ‰‹åŠ¿: {multimodal_input.gesture_data['gesture']} (ç½®ä¿¡åº¦: {multimodal_input.gesture_data['confidence']:.2f})")
    print(f"   ğŸ¤ è¯­éŸ³: '{multimodal_input.speech_data['text']}'")
    
    print("\nğŸ¤– æ­£åœ¨è°ƒç”¨DeepSeek API...")
    ai_response = deepseek_client.analyze_multimodal_data(multimodal_input)
    
    print(f"\nğŸ§  AIåˆ†æç»“æœ:")
    print(f"   ğŸ“‹ æ¨èæ“ä½œ: {ai_response.recommendation_text}")
    print(f"   ğŸ¯ ç½®ä¿¡åº¦: {ai_response.confidence:.2f}")
    print(f"   ğŸ’­ æ¨ç†è¿‡ç¨‹: {ai_response.reasoning}")
    print(f"   âš™ï¸ æ“ä½œæŒ‡ä»¤: {ai_response.action_code}")


def test_voice_command_scenario():
    """æµ‹è¯•è¯­éŸ³å‘½ä»¤åœºæ™¯"""
    print("\n" + "="*50)
    print("ğŸ¤ æµ‹è¯•åœºæ™¯: è¯­éŸ³å‘½ä»¤")
    
    # æ¨¡æ‹Ÿè¯­éŸ³å‘½ä»¤è¾“å…¥
    multimodal_input = MultimodalInput(
        gaze_data={
            "state": "center",
            "duration": 1.0,
            "deviation": "normal"
        },
        gesture_data={
            "gesture": "thumbs_up",
            "confidence": 0.85,
            "intent": "confirm"
        },
        speech_data={
            "text": "å¯¼èˆªåˆ°å®¶",
            "intent": "navigation",
            "emotion": "neutral"
        },
        timestamp=time.time(),
        duration=2.0
    )
    
    print("ğŸ“Š è¾“å…¥æ•°æ®:")
    print(f"   ğŸ‘ çœ¼åŠ¨: {multimodal_input.gaze_data['state']}")
    print(f"   ğŸ– æ‰‹åŠ¿: {multimodal_input.gesture_data['gesture']} (ç½®ä¿¡åº¦: {multimodal_input.gesture_data['confidence']:.2f})")
    print(f"   ğŸ¤ è¯­éŸ³: '{multimodal_input.speech_data['text']}'")
    
    print("\nğŸ¤– æ­£åœ¨è°ƒç”¨DeepSeek API...")
    ai_response = deepseek_client.analyze_multimodal_data(multimodal_input)
    
    print(f"\nğŸ§  AIåˆ†æç»“æœ:")
    print(f"   ğŸ“‹ æ¨èæ“ä½œ: {ai_response.recommendation_text}")
    print(f"   ğŸ¯ ç½®ä¿¡åº¦: {ai_response.confidence:.2f}")
    print(f"   ğŸ’­ æ¨ç†è¿‡ç¨‹: {ai_response.reasoning}")
    print(f"   âš™ï¸ æ“ä½œæŒ‡ä»¤: {ai_response.action_code}")


def test_emergency_scenario():
    """æµ‹è¯•ç´§æ€¥æƒ…å†µåœºæ™¯"""
    print("\n" + "="*50)
    print("ğŸš¨ æµ‹è¯•åœºæ™¯: ç´§æ€¥æƒ…å†µå¤„ç†")
    
    # æ¨¡æ‹Ÿç´§æ€¥æƒ…å†µï¼šé•¿æ—¶é—´åˆ†å¿ƒ + æ— å“åº”
    multimodal_input = MultimodalInput(
        gaze_data={
            "state": "left",  # æŒç»­åç¦»
            "duration": 8.0,  # å¾ˆé•¿æ—¶é—´
            "deviation": "severe"
        },
        gesture_data={
            "gesture": "none",
            "confidence": 0.0,
            "intent": "unknown"
        },
        speech_data={
            "text": "",
            "intent": "unknown",
            "emotion": "neutral"
        },
        timestamp=time.time(),
        duration=8.0
    )
    
    print("ğŸ“Š è¾“å…¥æ•°æ®:")
    print(f"   ğŸ‘ çœ¼åŠ¨: {multimodal_input.gaze_data['state']} ({multimodal_input.gaze_data['duration']:.1f}s)")
    print(f"   ğŸ– æ‰‹åŠ¿: {multimodal_input.gesture_data['gesture']}")
    print(f"   ğŸ¤ è¯­éŸ³: '{multimodal_input.speech_data['text']}'")
    
    print("\nğŸ¤– æ­£åœ¨è°ƒç”¨DeepSeek API...")
    ai_response = deepseek_client.analyze_multimodal_data(multimodal_input)
    
    print(f"\nğŸ§  AIåˆ†æç»“æœ:")
    print(f"   ğŸ“‹ æ¨èæ“ä½œ: {ai_response.recommendation_text}")
    print(f"   ğŸ¯ ç½®ä¿¡åº¦: {ai_response.confidence:.2f}")
    print(f"   ğŸ’­ æ¨ç†è¿‡ç¨‹: {ai_response.reasoning}")
    print(f"   âš™ï¸ æ“ä½œæŒ‡ä»¤: {ai_response.action_code}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ§ª DeepSeek API å¤šæ¨¡æ€èåˆæµ‹è¯• - æ— TTSç‰ˆæœ¬")
    print("=" * 60)
    
    try:
        # æµ‹è¯•å®Œæ•´çš„åˆ†å¿ƒé©¾é©¶æµç¨‹
        test_distraction_complete_scenario()
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´
        time.sleep(1)
        
        # æµ‹è¯•ä»…æ‰‹åŠ¿ç¡®è®¤
        test_distraction_gesture_only()
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´
        time.sleep(1)
        
        # æµ‹è¯•è¯­éŸ³å‘½ä»¤åœºæ™¯
        test_voice_command_scenario()
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´
        time.sleep(1)
        
        # æµ‹è¯•ç´§æ€¥æƒ…å†µ
        test_emergency_scenario()
        
        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æµ‹è¯•åœºæ™¯å®Œæˆ")
        print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
        print("   ğŸš¨ åˆ†å¿ƒæ£€æµ‹ â†’ è¯­éŸ³+æ‰‹åŠ¿ç¡®è®¤ â†’ æ¢å¤æ­£å¸¸")
        print("   ğŸ– åˆ†å¿ƒæ£€æµ‹ â†’ ä»…æ‰‹åŠ¿ç¡®è®¤ â†’ æ¢å¤æ­£å¸¸")
        print("   ğŸ¤ æ­£å¸¸è¯­éŸ³å‘½ä»¤ â†’ å¤šæ¨¡æ€ç¡®è®¤ â†’ æ‰§è¡Œæ“ä½œ")
        print("   ğŸš¨ ç´§æ€¥æƒ…å†µ â†’ é•¿æ—¶é—´åˆ†å¿ƒ â†’ å‡çº§è­¦æŠ¥")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 
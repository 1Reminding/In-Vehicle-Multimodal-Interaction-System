import cv2
import mediapipe as mp
from gaze_tracking import GazeTracking
import time
import dlib
import numpy as np
from imutils import face_utils
from scipy.spatial import distance as dist

# åˆå§‹åŒ– MediaPipe Hands
mp_drawing = mp.solutions.drawing_utils
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.75,
    min_tracking_confidence=0.75)

# åˆå§‹åŒ–å¤´éƒ¨å§¿æ€æ£€æµ‹
det = dlib.get_frontal_face_detector()
pre = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
(lS, lE) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
(rS, rE) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]
(mS, mE) = face_utils.FACIAL_LANDMARKS_IDXS["mouth"]

def get_y(hand_landmarks, landmark_enum):
    return hand_landmarks.landmark[landmark_enum].y

def get_x(hand_landmarks, landmark_enum):
    return hand_landmarks.landmark[landmark_enum].x

def head_pose(shape, size):
    """è®¡ç®—å¤´éƒ¨å§¿æ€"""
    model_pts = np.float32([
        (0.0,   0.0,   0.0),      # é¼»å°– 30
        (0.0,  -330.0, -65.0),    # ä¸‹å·´ 8
        (-225.0, 170.0, -135.0),  # å·¦çœ¼è§’ 36
        (225.0, 170.0, -135.0),   # å³çœ¼è§’ 45
        (-150.0, -150.0, -125.0), # å·¦å˜´è§’ 48
        (150.0, -150.0, -125.0)   # å³å˜´è§’ 54
    ])

    image_pts = np.float32([
        shape[30],  # é¼»å°–
        shape[8],   # ä¸‹å·´
        shape[36],  # å·¦çœ¼è§’
        shape[45],  # å³çœ¼è§’
        shape[48],  # å·¦å˜´è§’
        shape[54]   # å³å˜´è§’
    ])

    h, w = size
    focal = w
    center = (w / 2, h / 2)
    camera_mtx = np.array([[focal, 0, center[0]],
                           [0, focal, center[1]],
                           [0, 0, 1]], dtype=np.float32)
    dist_coef = np.zeros((4, 1))

    ok, rvec, tvec = cv2.solvePnP(model_pts, image_pts, camera_mtx,
                                  dist_coef, flags=cv2.SOLVEPNP_ITERATIVE)
    if not ok:
        return None

    rot_mat, _ = cv2.Rodrigues(rvec)
    pose_mat = cv2.hconcat((rot_mat, tvec))
    _, _, _, _, _, _, euler = cv2.decomposeProjectionMatrix(pose_mat)
    pitch, yaw, roll = [x[0] for x in euler]  # degree
    return yaw, pitch, roll

def recognize_gesture(hand_landmarks):
    # å…³é”®ç‚¹æšä¸¾
    lm_thumb_tip = mp_hands.HandLandmark.THUMB_TIP
    lm_thumb_ip = mp_hands.HandLandmark.THUMB_IP
    lm_thumb_mcp = mp_hands.HandLandmark.THUMB_MCP
    
    lm_index_tip = mp_hands.HandLandmark.INDEX_FINGER_TIP
    lm_index_pip = mp_hands.HandLandmark.INDEX_FINGER_PIP
    lm_index_mcp = mp_hands.HandLandmark.INDEX_FINGER_MCP
    
    lm_middle_tip = mp_hands.HandLandmark.MIDDLE_FINGER_TIP
    lm_middle_pip = mp_hands.HandLandmark.MIDDLE_FINGER_PIP
    lm_middle_mcp = mp_hands.HandLandmark.MIDDLE_FINGER_MCP
    
    lm_ring_tip = mp_hands.HandLandmark.RING_FINGER_TIP
    lm_ring_pip = mp_hands.HandLandmark.RING_FINGER_PIP
    lm_ring_mcp = mp_hands.HandLandmark.RING_FINGER_MCP
    
    lm_pinky_tip = mp_hands.HandLandmark.PINKY_TIP
    lm_pinky_pip = mp_hands.HandLandmark.PINKY_PIP
    lm_pinky_mcp = mp_hands.HandLandmark.PINKY_MCP

    # è·å–å„å…³é”®ç‚¹åæ ‡
    thumb_tip_y, thumb_ip_y, thumb_mcp_y = get_y(hand_landmarks, lm_thumb_tip), get_y(hand_landmarks, lm_thumb_ip), get_y(hand_landmarks, lm_thumb_mcp)
    index_tip_y, index_pip_y, index_mcp_y = get_y(hand_landmarks, lm_index_tip), get_y(hand_landmarks, lm_index_pip), get_y(hand_landmarks, lm_index_mcp)
    middle_tip_y, middle_pip_y, middle_mcp_y = get_y(hand_landmarks, lm_middle_tip), get_y(hand_landmarks, lm_middle_pip), get_y(hand_landmarks, lm_middle_mcp)
    ring_tip_y, ring_pip_y, ring_mcp_y = get_y(hand_landmarks, lm_ring_tip), get_y(hand_landmarks, lm_ring_pip), get_y(hand_landmarks, lm_ring_mcp)
    pinky_tip_y, pinky_pip_y, pinky_mcp_y = get_y(hand_landmarks, lm_pinky_tip), get_y(hand_landmarks, lm_pinky_pip), get_y(hand_landmarks, lm_pinky_mcp)

    # 1. è¯†åˆ«"ç«–èµ·å¤§æ‹‡æŒ‡"
    thumb_points_up = (thumb_tip_y < thumb_ip_y and thumb_ip_y < thumb_mcp_y)
    other_fingers_curled = (index_tip_y > index_pip_y and
                            middle_tip_y > middle_pip_y and
                            ring_tip_y > ring_pip_y and
                            pinky_tip_y > pinky_pip_y)
    thumb_dominant = thumb_tip_y < index_mcp_y 

    if thumb_points_up and other_fingers_curled and thumb_dominant:
        return "ç«–èµ·å¤§æ‹‡æŒ‡"

    # 2. è¯†åˆ«"æ‘‡æ‰‹"ï¼ˆå¼ å¼€æ‰‹æŒï¼‰
    thumb_extended = (thumb_tip_y < thumb_ip_y and thumb_tip_y < thumb_mcp_y)
    index_extended = (index_tip_y < index_pip_y and index_tip_y < index_mcp_y)
    middle_extended = (middle_tip_y < middle_pip_y and middle_tip_y < middle_mcp_y)
    ring_extended = (ring_tip_y < ring_pip_y and ring_tip_y < ring_mcp_y)
    pinky_extended = (pinky_tip_y < pinky_pip_y and pinky_tip_y < pinky_mcp_y)

    if thumb_extended and index_extended and middle_extended and ring_extended and pinky_extended:
        return "æ‘‡æ‰‹"

    # 3. è¯†åˆ«"æ¡æ‹³"
    index_fist_curled = (index_tip_y > index_pip_y and index_tip_y > index_mcp_y)
    middle_fist_curled = (middle_tip_y > middle_pip_y and middle_tip_y > middle_mcp_y)
    ring_fist_curled = (ring_tip_y > ring_pip_y and ring_tip_y > ring_mcp_y)
    pinky_fist_curled = (pinky_tip_y > pinky_pip_y and pinky_tip_y > pinky_mcp_y)
    thumb_fist_curled = (thumb_tip_y > thumb_ip_y and thumb_tip_y > thumb_mcp_y)

    if index_fist_curled and middle_fist_curled and ring_fist_curled and pinky_fist_curled and thumb_fist_curled:
        return "æ¡æ‹³"
        
    return None

def main():
    # åˆå§‹åŒ–çœ¼ç¥è¿½è¸ªå™¨
    gaze = GazeTracking()
    
    # æ‰“å¼€æ‘„åƒå¤´
    webcam = cv2.VideoCapture(0)
    
    # åˆå§‹åŒ–çŠ¶æ€è·Ÿè¸ªå˜é‡
    current_state = "center"
    state_start_time = time.time()
    previous_distracted = False
    last_gesture = None
    waiting_for_thumbs_up = False
    is_distracted = False
    mouth_open = False  # æ·»åŠ å˜´éƒ¨çŠ¶æ€è·Ÿè¸ª
    
    # è®¾ç½®åˆ†å¿ƒé˜ˆå€¼ï¼ˆç§’ï¼‰
    DISTRACTION_THRESHOLD = {
        "blink": 1.0,
        "left": 3.0,
        "right": 3.0,
    }
    
    # å¤´éƒ¨å§¿æ€æ£€æµ‹ç›¸å…³å˜é‡
    CALIB_SECS = 2               # åŸºçº¿é‡‡æ ·æ—¶é•¿
    FPS_GUESS = 30
    YAW_THRESH = 10              # Â°  |Yaw|>é˜ˆå€¼ â†’ å·¦/å³
    PITCH_DELTA = 6              # Â° ä½å¤´ç›¸å¯¹ä¸‹é™è§’
    PITCH_FRAMES = 1
    
    # åŸºçº¿æ ¡å‡†ç›¸å…³å˜é‡
    ear_sum = pitch_sum = 0
    sample_cnt = 0
    calibrated = False
    ear0 = pitch0 = None
    
    # å¤´éƒ¨å§¿æ€ç›¸å…³å˜é‡
    yaw_dir = None
    yaw_flag = 0
    pitch_down_frames = 0
    last_head_time = time.time()
    
    # æ·»åŠ è®¡æ•°å™¨
    tot_mouth = 0
    tot_shake = 0
    tot_nod = 0
    prev_state = (-1, -1, -1)  # ç”¨äºè·Ÿè¸ªçŠ¶æ€å˜åŒ–
    last_head_time = time.time()

    while True:
        # è¯»å–æ‘„åƒå¤´å¸§
        ret, frame = webcam.read()
        if not ret:
            print("æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
            break

        # å…ˆè¿›è¡Œé•œé¢ç¿»è½¬ï¼Œå†å¤„ç†æ‰€æœ‰åŠŸèƒ½
        frame = cv2.flip(frame, 1)

        # å¤„ç†çœ¼ç¥è¿½è¸ª
        gaze.refresh(frame)
        frame = gaze.annotated_frame()

        # å¤„ç†å¤´éƒ¨å§¿æ€æ£€æµ‹
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        rects = det(gray, 0)
        head_pose_text = "æœªæ£€æµ‹åˆ°äººè„¸"
        head_pose_color = (0, 0, 255)
        
        if rects:
            shape = face_utils.shape_to_np(pre(gray, rects[0]))
            pose = head_pose(shape, frame.shape[:2])
            
            if pose:
                yaw, pitch, roll = pose
                
                # åŸºçº¿æ ¡å‡†
                if not calibrated:
                    pitch_sum += pitch
                    sample_cnt += 1
                    if sample_cnt >= CALIB_SECS * FPS_GUESS:
                        pitch0 = pitch_sum / sample_cnt
                        calibrated = True
                        print(f"ğŸ“ åŸºçº¿å®Œæˆï¼špitch0={pitch0:.1f}Â°")
                    continue
                
                # å¤´éƒ¨å§¿æ€æ£€æµ‹
                head_pose_status = "æ­£å¸¸"
                head_pose_color = (0, 255, 0)
                
                # æ‘‡å¤´æ£€æµ‹
                if abs(yaw) > YAW_THRESH:
                    cur = 'L' if yaw > 0 else 'R'
                    if yaw_dir != cur:
                        yaw_flag += 1
                        yaw_dir = cur
                    if yaw_flag >= 2:
                        tot_shake += 1
                        yaw_flag = 0
                        head_pose_status = "æ‘‡å¤´"
                        head_pose_color = (0, 0, 255)
                
                # ç‚¹å¤´æ£€æµ‹
                if pitch < pitch0 - PITCH_DELTA:
                    pitch_down_frames += 1
                else:
                    if pitch_down_frames >= PITCH_FRAMES:
                        tot_nod += 1
                        head_pose_status = "ç‚¹å¤´"
                        head_pose_color = (0, 165, 255)
                    pitch_down_frames = 0
                
                # æ›´æ–°çŠ¶æ€å¹¶æ‰“å°
                state = (tot_mouth, tot_shake, tot_nod)
                if state != prev_state:
                    print(f"Mouth={tot_mouth}  Shake={tot_shake}  Nod={tot_nod}")
                    prev_state = state
                
                head_pose_text = f"å¤´éƒ¨å§¿æ€: {head_pose_status} (Pitch: {pitch:.1f}, Yaw: {yaw:.1f})"

        # æ£€æµ‹å˜´éƒ¨åŠ¨ä½œ
        if rects:
            shape = face_utils.shape_to_np(pre(gray, rects[0]))
            mouth = shape[mS:mE]
            mar = dist.euclidean(mouth[2], mouth[9]) + dist.euclidean(mouth[4], mouth[7])
            mar = mar / (2.0 * dist.euclidean(mouth[0], mouth[6]))
            
            if mar > 0.5:  # å¼ å˜´é˜ˆå€¼
                if not mouth_open:
                    mouth_open = True
                    tot_mouth += 1
                    # æ›´æ–°çŠ¶æ€å¹¶æ‰“å°
                    state = (tot_mouth, tot_shake, tot_nod)
                    if state != prev_state:
                        print(f"Mouth={tot_mouth}  Shake={tot_shake}  Nod={tot_nod}")
                        prev_state = state
            else:
                mouth_open = False

        # å¤„ç†æ‰‹åŠ¿è¯†åˆ«
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(frame_rgb)

        # çœ¼éƒ¨è¿½è¸ªçŠ¶æ€å¤„ç†
        new_state = ""
        if gaze.is_blinking():
            new_state = "blink"
        elif gaze.is_right():
            new_state = "right"
        elif gaze.is_left():
            new_state = "left"
        elif gaze.is_center():
            new_state = "center"
            
        if new_state != current_state:
            current_state = new_state
            state_start_time = time.time()
        
        duration = time.time() - state_start_time
        
        # ä¿®æ”¹åˆ†å¿ƒçŠ¶æ€å¤„ç†é€»è¾‘
        if not is_distracted and current_state in DISTRACTION_THRESHOLD and duration > DISTRACTION_THRESHOLD[current_state]:
            is_distracted = True
            waiting_for_thumbs_up = True
            print(f"è­¦å‘Šï¼šæ£€æµ‹åˆ°åˆ†å¿ƒï¼å½“å‰çŠ¶æ€: {current_state}")
            print("è¯·ç«–èµ·å¤§æ‹‡æŒ‡ä»¥ç¡®è®¤æ‚¨å·²æ¢å¤æ³¨æ„åŠ›")
        
        # æ‰‹åŠ¿è¯†åˆ«å¤„ç†
        current_gesture = None
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(
                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                
                current_gesture = recognize_gesture(hand_landmarks)
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç«–èµ·å¤§æ‹‡æŒ‡æ¥è§£é™¤åˆ†å¿ƒçŠ¶æ€
                if waiting_for_thumbs_up and current_gesture == "ç«–èµ·å¤§æ‹‡æŒ‡":
                    is_distracted = False
                    waiting_for_thumbs_up = False
                    print("å·²ç¡®è®¤æ³¨æ„åŠ›æ¢å¤ï¼Œè§£é™¤åˆ†å¿ƒçŠ¶æ€")
                    last_gesture = current_gesture  # æ›´æ–°ä¸Šä¸€æ¬¡çš„æ‰‹åŠ¿çŠ¶æ€
                    
        # åªåœ¨æ‰‹åŠ¿çŠ¶æ€å‘ç”Ÿå˜åŒ–æ—¶è¾“å‡º
        if current_gesture != last_gesture:
            if current_gesture:
                print(f"è¯†åˆ«åˆ°çš„æ‰‹åŠ¿: {current_gesture}")
            elif last_gesture:  # åªæœ‰åœ¨ä¹‹å‰æœ‰æ‰‹åŠ¿çš„æƒ…å†µä¸‹æ‰æç¤ºæ‰‹åŠ¿ç»“æŸ
                print("æ‰‹åŠ¿ç»“æŸ")
            last_gesture = current_gesture

        # åœ¨ç”»é¢ä¸Šæ˜¾ç¤ºä¿¡æ¯
        gaze_text = f"{current_state} ({duration:.1f}ç§’)"
        if is_distracted:
            gaze_text += " (åˆ†å¿ƒ - è¯·ç«–èµ·å¤§æ‹‡æŒ‡ç¡®è®¤æ¢å¤æ³¨æ„åŠ›)"
        cv2.putText(frame, gaze_text, (90, 60), cv2.FONT_HERSHEY_DUPLEX, 1.6, (147, 58, 31), 2)

        if current_gesture:
            cv2.putText(frame, f"æ‰‹åŠ¿: {current_gesture}", (90, 120), cv2.FONT_HERSHEY_DUPLEX, 1.0, (0, 255, 0), 2)

        # æ˜¾ç¤ºç³å­”ä½ç½®
        left_pupil = gaze.pupil_left_coords()
        right_pupil = gaze.pupil_right_coords()
        if left_pupil is not None:
            cv2.putText(frame, f"å·¦ç³å­”: {left_pupil}", (90, 180), cv2.FONT_HERSHEY_DUPLEX, 0.9, (147, 58, 31), 1)
        if right_pupil is not None:
            cv2.putText(frame, f"å³ç³å­”: {right_pupil}", (90, 220), cv2.FONT_HERSHEY_DUPLEX, 0.9, (147, 58, 31), 1)
            
        # æ˜¾ç¤ºå¤´éƒ¨å§¿æ€ä¿¡æ¯
        cv2.putText(frame, head_pose_text, (90, 260), cv2.FONT_HERSHEY_DUPLEX, 0.9, head_pose_color, 1)

        # åœ¨ç”»é¢ä¸‹æ–¹æ˜¾ç¤ºè®¡æ•°
        count_text = f"Mouth={tot_mouth}  Shake={tot_shake}  Nod={tot_nod}"
        cv2.putText(frame, count_text, (10, 460), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        # æ˜¾ç¤ºå¤„ç†åçš„å¸§
        cv2.imshow("ç»„åˆæ¼”ç¤º", frame)

        if cv2.waitKey(1) & 0xFF == 27:  # æŒ‰ESCé€€å‡º
            break

    webcam.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
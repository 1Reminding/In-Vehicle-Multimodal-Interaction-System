import time
import math
import cv2
import dlib
import numpy as np
from imutils import face_utils
from scipy.spatial import distance as dist

# ---------------- å·¥å…·å‡½æ•° ----------------
def eye_aspect_ratio(eye):
    """çœ¨çœ¼æ¯”ä¾‹ EAR"""
    A = dist.euclidean(eye[1], eye[5])
    B = dist.euclidean(eye[2], eye[4])
    C = dist.euclidean(eye[0], eye[3])
    return (A + B) / (2.0 * C)

def head_pose(shape, size):
    """
    solvePnP â†’ è¿”å› (yaw, pitch, roll) in degree.
    yaw>0: å·¦è½¬ï¼Œpitch>0: æŠ¬å¤´
    """

    model_pts = np.float32([
        (0.0,   0.0,   0.0),      # é¼»å°– 30
        (0.0,  -330.0, -65.0),    # ä¸‹å·´ 8
        (-225.0, 170.0, -135.0),  # å·¦çœ¼è§’ 36
        (225.0, 170.0, -135.0),   # å³çœ¼è§’ 45
        (-150.0, -150.0, -125.0), # å·¦å˜´è§’ 48
        (150.0, -150.0, -125.0)   # å³å˜´è§’ 54
    ])

    image_pts = np.float32([
        shape[30],  # é¼»å°–
        shape[8],   # ä¸‹å·´
        shape[36],  # å·¦çœ¼è§’
        shape[45],  # å³çœ¼è§’
        shape[48],  # å·¦å˜´è§’
        shape[54]   # å³å˜´è§’
    ])

    h, w = size
    focal = w
    center = (w / 2, h / 2)
    camera_mtx = np.array([[focal, 0, center[0]],
                           [0, focal, center[1]],
                           [0, 0, 1]], dtype=np.float32)
    dist_coef = np.zeros((4, 1))

    ok, rvec, tvec = cv2.solvePnP(model_pts, image_pts, camera_mtx,
                                  dist_coef, flags=cv2.SOLVEPNP_ITERATIVE)
    if not ok:
        return None

    rot_mat, _ = cv2.Rodrigues(rvec)
    pose_mat = cv2.hconcat((rot_mat, tvec))
    _, _, _, _, _, _, euler = cv2.decomposeProjectionMatrix(pose_mat)
    pitch, yaw, roll = [x[0] for x in euler]  # degree
    return yaw, pitch, roll


# ---------------- ä¸»æ£€æµ‹ ----------------
def live_detect():
    # ------------- å¸¸é‡ -------------
    CALIB_SECS   = 2               # åŸºçº¿é‡‡æ ·æ—¶é•¿
    FPS_GUESS    = 30
    YAW_THRESH   = 10              # Â°  |Yaw|>é˜ˆå€¼ â†’ å·¦/å³
    PITCH_DELTA  = 6              # Â° ä½å¤´ç›¸å¯¹ä¸‹é™è§’
    PITCH_FRAMES = 1

    # ------------- è®¡æ•°å™¨ -------------
    pitch_sum = 0
    sample_cnt = 0
    calibrated = False
    pitch0 = None

    yaw_dir = None
    yaw_flag = 0
    tot_shake = 0
    pitch_down_frames = 0
    tot_nod = 0
    prev_state = (-1, -1)  # åªä¿ç•™æ‘‡å¤´å’Œç‚¹å¤´çŠ¶æ€
    done = False

    # ------------- æ¨¡å‹ -------------
    det = dlib.get_frontal_face_detector()
    pre = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
    (lS, lE) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
    (rS, rE) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]

    # ------------- æ‘„åƒå¤´ -------------
    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return

    print("æ‘„åƒå¤´å·²æ‰“å¼€ï¼ŒæŒ‰ Q é€€å‡º...")

    while True:
        ret, frame = cap.read()
        if not ret:
            continue

        frame = cv2.resize(frame, (640, 480))
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        rects = det(gray, 0)
        if not rects:
            cv2.imshow("Live", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            continue

        shape = face_utils.shape_to_np(pre(gray, rects[0]))

        ear_val = (eye_aspect_ratio(shape[lS:lE]) +
                   eye_aspect_ratio(shape[rS:rE])) / 2.0
        pose = head_pose(shape, frame.shape[:2])

        # ---------- åŸºçº¿æ ¡å‡† ----------
        if not calibrated and pose:
            pitch_sum += pose[1]
            sample_cnt += 1
            if sample_cnt >= CALIB_SECS * FPS_GUESS:
                pitch0 = pitch_sum / sample_cnt
                calibrated = True
                print(f"ğŸ“ åŸºçº¿å®Œæˆï¼špitch0={pitch0:.1f}Â°")
            cv2.imshow("Live", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            continue

        # ---------------- æ‘‡å¤´ ----------------
        yaw, pitch, _ = pose
        if abs(yaw) > YAW_THRESH:
            cur = 'L' if yaw > 0 else 'R'
            if yaw_dir != cur:
                yaw_flag += 1
                yaw_dir = cur
            if yaw_flag >= 2:
                tot_shake += 1
                yaw_flag = 0

        # ---------------- ç‚¹å¤´ ----------------
        if pitch < pitch0 - PITCH_DELTA:
            pitch_down_frames += 1
        else:
            if pitch_down_frames >= PITCH_FRAMES:
                tot_nod += 1
            pitch_down_frames = 0

        # ---------------- ç»ˆç«¯è¾“å‡º ----------------
        state = (tot_shake, tot_nod)  # åªä¿ç•™æ‘‡å¤´å’Œç‚¹å¤´è®¡æ•°
        if state != prev_state:
            print(f"Shake={tot_shake}  Nod={tot_nod}", flush=True)
            prev_state = state

        # åœ¨ç”»é¢ä¸‹æ–¹æ˜¾ç¤ºè®¡æ•°
        text = f"Shake={tot_shake}  Nod={tot_nod}"
        cv2.putText(frame, text, (10, 460), cv2.FONT_HERSHEY_SIMPLEX,
                    0.7, (0, 255, 0), 2)

        cv2.imshow("Live", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    live_detect()

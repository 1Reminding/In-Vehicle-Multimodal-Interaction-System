# import whisper
# import torch
# import sounddevice as sd
# import webrtcvad
# import collections
# import wave
# import tempfile
# import os
# from modules.ai.multimodal_collector import multimodal_collector

# # ASR
# # å…¨å±€åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
# DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# MODEL = whisper.load_model("turbo").to(DEVICE)

# def transcribe_file(file_path: str) -> str:
#     """
#     è¯†åˆ«æŒ‡å®šéŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›è½¬å†™åçš„çº¯æ–‡æœ¬ã€‚
#     æ”¯æŒ wav/mp3/flac ç­‰æ ¼å¼ï¼Œè‡ªåŠ¨ä½¿ç”¨ GPUï¼ˆè‹¥å¯ç”¨ï¼‰æˆ– CPUã€‚
#     """
#     # Whisper çš„ transcribe æ–¹æ³•å†…éƒ¨ä¼šè°ƒç”¨ ffmpeg è½¬ç 
#     result = MODEL.transcribe(file_path, fp16=(DEVICE == "cuda"))
#     return result["text"].strip()


# # main
# RATE = 16000
# FRAME_DURATION = 30
# FRAME_SIZE = int(RATE * FRAME_DURATION / 1000)
# CHANNELS = 1

# vad = webrtcvad.Vad(3)

# def listen_loop(silence_limit=0.8):
#     buffer = collections.deque(maxlen=int(silence_limit * 1000 / FRAME_DURATION))
#     voiced_frames = []
#     is_recording = False

#     with sd.RawInputStream(samplerate=RATE,
#                            blocksize=FRAME_SIZE,
#                            dtype='int16',
#                            channels=CHANNELS) as stream:
#         print("â†’ å·²è¿›å…¥åŒæ­¥ç›‘å¬æ¨¡å¼ï¼Œç­‰å¾…æŒ‡ä»¤â€¦")
#         while True:
#             frame, _ = stream.read(FRAME_SIZE)
#             frame_bytes = bytes(frame)
#             is_speech = vad.is_speech(frame_bytes, RATE)

#             if is_speech:
#                 if not is_recording:
#                     print("[å½•éŸ³å¼€å§‹]")
#                     is_recording = True
#                 voiced_frames.append(frame_bytes)
#                 buffer.append(True)
#             else:
#                 buffer.append(False)
#                 # åªæœ‰å½“å·²ç»å¼€å§‹å½•åˆ¶ï¼Œä¸”æ£€æµ‹åˆ°è¿ç»­é™éŸ³æ—¶ï¼Œæ‰è§¦å‘å¤„ç†
#                 if is_recording and not any(buffer):
#                     print("[å½•éŸ³ç»“æŸï¼Œå¼€å§‹åŒæ­¥å¤„ç†]")
#                     # å†™ä¸´æ—¶ wav
#                     tmp = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
#                     with wave.open(tmp.name, 'wb') as wf:
#                         wf.setnchannels(CHANNELS)
#                         wf.setsampwidth(2)
#                         wf.setframerate(RATE)
#                         wf.writeframes(b''.join(voiced_frames))

#                     tmp.close()

#                     try:
#                         # å¯¹ä¸´æ—¶æ–‡ä»¶è¿›è¡Œè¯†åˆ«
#                         text = transcribe_file(tmp.name)                       

#                         if text:
#                             # ç›´æ¥æ‰§è¡Œä¸»çº¿ç¨‹åŸæœ¬åšçš„äº‹æƒ…
#                             print(f"ğŸ¤ è¯­éŸ³è¯†åˆ«: '{text}'")
                            
#                             # æ›´æ–°å¤šæ¨¡æ€æ”¶é›†å™¨
#                             speech_data = {
#                                 "text": text,
#                             }
#                             multimodal_collector.update_speech_data(speech_data)
                    
#                     finally:
#                         # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
#                         try:
#                             os.unlink(tmp.name)
#                         except:
#                             pass

#                     # é‡ç½®çŠ¶æ€ï¼Œå›åˆ°ç›‘å¬
#                     voiced_frames.clear()
#                     buffer.clear()
#                     is_recording = False
#                     print("â†’ å›åˆ°ç›‘å¬çŠ¶æ€â€¦")

# # def main():
# #     try:
# #         listen_loop(silence_limit=0.8)
# #     except KeyboardInterrupt:
# #         print("é€€å‡ºç›‘å¬ã€‚")

# # if __name__ == "__main__":
# #     main()

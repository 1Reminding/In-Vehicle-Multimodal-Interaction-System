#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡æ€æ•°æ®æ”¶é›†å™¨æ¨¡å—

è´Ÿè´£æ”¶é›†å’Œæ•´åˆæ¥è‡ªä¸åŒæ¨¡æ€çš„æ•°æ®
"""

import time
import threading
from typing import Dict, Any, Optional, Callable
from dataclasses import dataclass, field
from collections import deque

from modules.ai.deepseek_client import MultimodalInput


@dataclass
class GazeState:
    """çœ¼åŠ¨çŠ¶æ€"""
    state: str  # "left", "right", "center"
    start_time: float
    duration: float = 0.0
    deviation_level: str = "normal"  # "normal", "mild", "severe"


@dataclass
class GestureState:
    """æ‰‹åŠ¿çŠ¶æ€"""
    gesture: str
    confidence: float
    intent: str = "unknown"
    timestamp: float = field(default_factory=time.time)


@dataclass
class SpeechState:
    """è¯­éŸ³çŠ¶æ€"""
    text: str
    intent: str = "unknown"  # å°†é€šè¿‡ _infer_speech_intent å¡«å……
    emotion: str = "neutral"
    timestamp: float = field(default_factory=time.time)


class MultimodalCollector:
    """å¤šæ¨¡æ€æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self, gaze_threshold: float = 3.0):
        self.gaze_threshold = gaze_threshold  # çœ¼åŠ¨åç¦»é˜ˆå€¼ï¼ˆç§’ï¼‰
        
        # æ•°æ®çŠ¶æ€
        self.current_gaze_state: Optional[GazeState] = None
        self.current_gesture_state: Optional[GestureState] = None
        self.current_speech_state: Optional[SpeechState] = None
        
        # æ•°æ®å†å² (å¯é€‰ä¿ç•™ï¼Œå½“å‰é€»è¾‘ä¸å¼ºä¾èµ–)
        self.gaze_history = deque(maxlen=100)
        self.gesture_history = deque(maxlen=50)
        self.speech_history = deque(maxlen=20)
        
        # å›è°ƒå‡½æ•°
        self.on_multimodal_ready: Optional[Callable[[MultimodalInput], None]] = None
        
        # çº¿ç¨‹é”
        self._lock = threading.Lock()
        
        # åˆ†å¿ƒçŠ¶æ€ç®¡ç†
        self.distraction_detected = False
        self.distraction_start_time: Optional[float] = None
        
        print("âœ… å¤šæ¨¡æ€æ•°æ®æ”¶é›†å™¨åˆå§‹åŒ–å®Œæˆ (æ–°ç‰ˆé€»è¾‘)")
    
    def update_gaze_data(self, gaze_data: Dict[str, Any]):
        """æ›´æ–°çœ¼åŠ¨æ•°æ®"""
        with self._lock:
            current_time = time.time()
            state = gaze_data.get("state", "center")
            
            if (self.current_gaze_state is None or 
                self.current_gaze_state.state != state):
                if self.current_gaze_state:
                    self.current_gaze_state.duration = current_time - self.current_gaze_state.start_time
                    self.gaze_history.append(self.current_gaze_state)
                
                self.current_gaze_state = GazeState(
                    state=state,
                    start_time=current_time
                )
                print(f"ğŸ‘ çœ¼åŠ¨çŠ¶æ€å˜åŒ–: {state}")

            if self.current_gaze_state:
                self.current_gaze_state.duration = current_time - self.current_gaze_state.start_time
                
                if state != "center":
                    if self.current_gaze_state.duration > self.gaze_threshold:
                        self.current_gaze_state.deviation_level = "severe"
                        if not self.distraction_detected:
                            self.distraction_detected = True
                            self.distraction_start_time = current_time
                            print(f"ğŸš¨ åˆ†å¿ƒé©¾é©¶æ£€æµ‹ï¼åç¦»æ—¶é—´: {self.current_gaze_state.duration:.1f}ç§’")
                            context = {
                                "type": "distraction_detected",
                                "gaze_duration": self.current_gaze_state.duration,
                                "reason": "gaze_deviation"
                            }
                            self._prepare_and_send_multimodal_data(context, triggered_by="gaze")
                    elif self.current_gaze_state.duration > self.gaze_threshold / 2:
                        self.current_gaze_state.deviation_level = "mild"
                    else:
                        self.current_gaze_state.deviation_level = "normal"
                else: # state == "center"
                    self.current_gaze_state.deviation_level = "normal"
                    # è§†çº¿å›åˆ°ä¸­å¿ƒï¼Œå¦‚æœä¹‹å‰æ˜¯åˆ†å¿ƒçŠ¶æ€ï¼Œåˆ†å¿ƒçŠ¶æ€ä¾ç„¶ä¿æŒï¼Œç­‰å¾…ç”¨æˆ·è¯­éŸ³/æ‰‹åŠ¿ç¡®è®¤æ¢å¤
                    if self.distraction_detected:
                        print("ğŸ‘€ è§†çº¿å·²å›åˆ°ä¸­å¿ƒï¼Œä½†ä»å¤„äºåˆ†å¿ƒçŠ¶æ€ã€‚ç­‰å¾…ç”¨æˆ·è¯­éŸ³æˆ–æ‰‹åŠ¿ç¡®è®¤æ¢å¤æ³¨æ„åŠ›ã€‚")
    
    def update_gesture_data(self, gesture_data: Dict[str, Any]):
        """æ›´æ–°æ‰‹åŠ¿æ•°æ®"""
        with self._lock:
            gesture = gesture_data.get("gesture")
            confidence = float(gesture_data.get("conf", 0.0))
            
            if gesture and confidence > 0.7:
                intent = self._infer_gesture_intent(gesture)
                self.current_gesture_state = GestureState(
                    gesture=gesture,
                    confidence=confidence,
                    intent=intent,
                    timestamp=time.time()
                )
                self.gesture_history.append(self.current_gesture_state)
                print(f"ğŸ– æ‰‹åŠ¿æ›´æ–°: {gesture} (ç½®ä¿¡åº¦: {confidence:.2f}, æ„å›¾: {intent})")
                
                context_type = "user_input"
                context_info = {"trigger": "gesture", "gesture": gesture, "intent": intent}

                if self.distraction_detected:
                    if self._is_confirmation_gesture(intent):
                        print(f"âœ… é€šè¿‡æ‰‹åŠ¿ '{gesture}' ç¡®è®¤ï¼Œé©¾é©¶å‘˜å·²æ¢å¤æ³¨æ„åŠ›")
                        self.distraction_detected = False
                        self.distraction_start_time = None
                        context_type = "attention_restored"
                        context_info["confirmed_by"] = "gesture"
                    else:
                        print(f"ğŸ‘‰ ç”¨æˆ·åœ¨åˆ†å¿ƒçŠ¶æ€ä¸‹è¾“å…¥æ‰‹åŠ¿: {gesture}")
                        context_type = "user_input_while_distracted"
                        # context_info["distraction_active"] = True # Already implied by context_type
                
                context_info["type"] = context_type
                self._prepare_and_send_multimodal_data(context_info, triggered_by="gesture")

    def update_speech_data(self, speech_data: Dict[str, Any]):
        """æ›´æ–°è¯­éŸ³æ•°æ®"""
        with self._lock:
            text = speech_data.get("text", "").strip()
            
            if text:
                emotion = self._infer_emotion(text)
                # ç®€å•æ¨æ–­è¯­éŸ³æ„å›¾ (å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•)
                speech_intent = "command" if not self._is_confirmation_speech(text) else "confirmation"

                self.current_speech_state = SpeechState(
                    text=text,
                    emotion=emotion,
                    intent=speech_intent, # æ–°å¢æ„å›¾
                    timestamp=time.time()
                )
                self.speech_history.append(self.current_speech_state)
                print(f"ğŸ¤ è¯­éŸ³æ›´æ–°: '{text}' (æƒ…æ„Ÿ: {emotion}, æ„å›¾: {speech_intent})")

                context_type = "user_input"
                context_info = {"trigger": "speech", "text": text, "emotion": emotion, "intent": speech_intent}

                if self.distraction_detected:
                    if self._is_confirmation_speech(text):
                        print(f"âœ… é€šè¿‡è¯­éŸ³ '{text}' ç¡®è®¤ï¼Œé©¾é©¶å‘˜å·²æ¢å¤æ³¨æ„åŠ›")
                        self.distraction_detected = False
                        self.distraction_start_time = None
                        context_type = "attention_restored"
                        context_info["confirmed_by"] = "speech"
                    else:
                        print(f"ğŸ—£ï¸ ç”¨æˆ·åœ¨åˆ†å¿ƒçŠ¶æ€ä¸‹è¾“å…¥è¯­éŸ³: {text}")
                        context_type = "user_input_while_distracted"
                        # context_info["distraction_active"] = True
                
                context_info["type"] = context_type
                self._prepare_and_send_multimodal_data(context_info, triggered_by="speech")

    def _infer_gesture_intent(self, gesture: str) -> str:
        """æ¨æ–­æ‰‹åŠ¿æ„å›¾"""
        gesture_intent_map = {
            "Thumbs Up": "ç¡®è®¤å·²å›åˆ°ä¸“æ³¨çŠ¶æ€",
            "Thumbs Down": "ä»ä¸ºåˆ†å¿ƒçŠ¶æ€",
            "OK": "ç¡®è®¤å·²å›åˆ°ä¸“æ³¨çŠ¶æ€", # Considered as confirmation
            "Close": "ä»ä¸ºåˆ†å¿ƒçŠ¶æ€", # Example action
            "Open": "æ’­æ”¾éŸ³ä¹", # Example action
            "Point": "æ‰“å¼€ç©ºè°ƒ" # Example action
        }
        return gesture_intent_map.get(gesture, "unknown")

    def _is_confirmation_gesture(self, intent: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç¡®è®¤æ‰‹åŠ¿æ„å›¾"""
        return intent in ["confirm", "ok"]

    def _infer_emotion(self, text: str) -> str:
        """æ¨æ–­æƒ…æ„Ÿå€¾å‘ï¼ˆç®€å•è§„åˆ™ï¼‰"""
        text_lower = text.lower()
        positive_keywords = ["å¥½", "æ˜¯", "ç¡®å®š", "åŒæ„", "å¯ä»¥", "è°¢è°¢", "æ£’", "ä¸é”™"]
        negative_keywords = ["ä¸", "æ²¡æœ‰", "æ‹’ç»", "ä¸è¦", "å–æ¶ˆ", "ç³Ÿç³•"]
        question_keywords = ["å—", "å‘¢", "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "?", "ï¼Ÿ"]
        
        if any(keyword in text_lower for keyword in positive_keywords):
            return "positive"
        elif any(keyword in text_lower for keyword in negative_keywords):
            return "negative"
        elif any(keyword in text_lower for keyword in question_keywords):
            return "questioning"
        else:
            return "neutral"
    
    def _is_confirmation_speech(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç¡®è®¤è¯­éŸ³"""
        confirmation_keywords = [
            "å·²æ³¨æ„", "æ³¨æ„é“è·¯", "çœ‹è·¯", "ä¸“å¿ƒ", "é›†ä¸­", "æ˜ç™½", "çŸ¥é“äº†", 
            "å¥½çš„", "æ”¶åˆ°", "ç¡®å®š", "æ˜¯çš„", "æ²¡é—®é¢˜", "æˆ‘å·²æ¢å¤æ³¨æ„åŠ›",
            "æ³¨æ„å‰æ–¹", "æˆ‘åœ¨çœ‹è·¯", "æ¢å¤æ³¨æ„", "æ˜ç™½äº†", "æˆ‘ä¼šæ³¨æ„",
            "è¡Œ", "å—¯", "ok"
        ]
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in confirmation_keywords)

    def _prepare_and_send_multimodal_data(self, context_info: Dict[str, Any], triggered_by: Optional[str] = None):
        """å‡†å¤‡å¹¶å‘é€å¤šæ¨¡æ€æ•°æ®"""
        current_time = time.time()
        
        gaze_d = self._get_gaze_data() # æ€»æ˜¯åŒ…å«çœ¼åŠ¨æ•°æ®
        speech_d = {"text": "", "intent": "unknown", "emotion": "neutral"}
        gesture_d = {"gesture": "none", "confidence": 0.0, "intent": "unknown"}

        # æ ‡è®°æ•°æ®æ˜¯å¦æ˜¯"æ–°çš„"æˆ–"è§¦å‘äº‹ä»¶çš„"
        # triggered_by ç”¨æ¥æŒ‡æ˜æ˜¯ä»€ä¹ˆç›´æ¥å¯¼è‡´äº†è¿™ä¸ªå‘é€äº‹ä»¶
        
        if triggered_by == "speech" and self.current_speech_state:
            speech_d = self._get_speech_data(consume=True) # è§¦å‘çš„è¯­éŸ³æ•°æ®ï¼Œæ¶ˆè€—æ‰
        elif self.current_speech_state and (current_time - self.current_speech_state.timestamp < 1.5): # æœ€è¿‘1.5ç§’å†…çš„è¯­éŸ³
            speech_d = self._get_speech_data(consume=False) # éè§¦å‘ä½†ä¼´éšçš„è¯­éŸ³ï¼Œä¸æ¶ˆè€—

        if triggered_by == "gesture" and self.current_gesture_state:
            gesture_d = self._get_gesture_data(consume=True) # è§¦å‘çš„æ‰‹åŠ¿æ•°æ®ï¼Œæ¶ˆè€—æ‰
        elif self.current_gesture_state and (current_time - self.current_gesture_state.timestamp < 2.0): # æœ€è¿‘2ç§’å†…çš„æ‰‹åŠ¿
            gesture_d = self._get_gesture_data(consume=False) # éè§¦å‘ä½†ä¼´éšçš„æ‰‹åŠ¿ï¼Œä¸æ¶ˆè€—
        
        # å¦‚æœæ˜¯å› åˆ†å¿ƒæ£€æµ‹è§¦å‘ï¼Œä¸”å½“æ—¶æœ‰è¾ƒæ–°çš„è¯­éŸ³/æ‰‹åŠ¿ï¼Œä¹Ÿä¸€å¹¶å¸¦ä¸Š
        if triggered_by == "gaze":
            if self.current_speech_state and (current_time - self.current_speech_state.timestamp < 1.5):
                speech_d = self._get_speech_data(consume=False) # ä¸æ¶ˆè€—ï¼Œå› ä¸ºä¸æ˜¯è¯­éŸ³ä¸»åŠ¨è§¦å‘
            if self.current_gesture_state and (current_time - self.current_gesture_state.timestamp < 2.0):
                gesture_d = self._get_gesture_data(consume=False) # ä¸æ¶ˆè€—

        multimodal_input = MultimodalInput(
            gaze_data=gaze_d,
            gesture_data=gesture_d,
            speech_data=speech_d,
            timestamp=current_time,
            duration=0.1,  # è¡¨ç¤ºç¬æ—¶äº‹ä»¶
            context=context_info
        )
        
        log_message = (
            f"ğŸ“‹ å‡†å¤‡å‘é€å¤šæ¨¡æ€æ•°æ® (ä¸Šä¸‹æ–‡: {context_info.get('type', 'N/A')}):\n"
            f"   - çœ¼åŠ¨: {multimodal_input.gaze_data['state']} (æŒç»­ {multimodal_input.gaze_data['duration']:.1f}s, "
            f"åˆ†å¿ƒ: {'æ˜¯' if multimodal_input.gaze_data['distraction_detected'] else 'å¦'})\n"
            f"   - æ‰‹åŠ¿: {multimodal_input.gesture_data['gesture']} (æ„å›¾: {multimodal_input.gesture_data['intent']})\n"
            f"   - è¯­éŸ³: '{multimodal_input.speech_data['text']}' (æ„å›¾: {multimodal_input.speech_data['intent']})"
        )
        print(log_message)
        
        if self.on_multimodal_ready:
            print(f"ğŸš€ è°ƒç”¨å¤šæ¨¡æ€æ•°æ®å°±ç»ªå›è°ƒ: {self.on_multimodal_ready.__qualname__ if hasattr(self.on_multimodal_ready, '__qualname__') else str(self.on_multimodal_ready)}")
            self.on_multimodal_ready(multimodal_input)
        else:
            print("âŒ é”™è¯¯: å¤šæ¨¡æ€æ•°æ®å°±ç»ªå›è°ƒ (on_multimodal_ready) æœªè®¾ç½®!")

    def _get_gaze_data(self) -> Dict[str, Any]:
        """è·å–å½“å‰çœ¼åŠ¨æ•°æ®"""
        if self.current_gaze_state:
            return {
                "state": self.current_gaze_state.state,
                "duration": float(self.current_gaze_state.duration),
                "deviation_level": self.current_gaze_state.deviation_level,
                "distraction_detected": self.distraction_detected,
            }
        return {
            "state": "center", 
            "duration": 0.0, 
            "deviation_level": "normal",
            "distraction_detected": self.distraction_detected, # å³ä½¿æ²¡æœ‰å½“å‰çœ¼åŠ¨çŠ¶æ€ï¼Œä¹Ÿè¦åæ˜ æ•´ä½“åˆ†å¿ƒçŠ¶æ€
        }
    
    def _get_gesture_data(self, consume: bool = False) -> Dict[str, Any]:
        """è·å–å½“å‰æ‰‹åŠ¿æ•°æ®"""
        data_to_return = {"gesture": "none", "confidence": 0.0, "intent": "unknown"}
        if self.current_gesture_state:
            data_to_return = {
                "gesture": self.current_gesture_state.gesture,
                "confidence": float(self.current_gesture_state.confidence),
                "intent": self.current_gesture_state.intent
            }
            if consume:
                print(f"ğŸ’¨ æ¶ˆè€—å·²å‘é€æ‰‹åŠ¿: {self.current_gesture_state.gesture}")
                self.current_gesture_state = None
        return data_to_return
    
    def _get_speech_data(self, consume: bool = False) -> Dict[str, Any]:
        """è·å–å½“å‰è¯­éŸ³æ•°æ®"""
        data_to_return = {"text": "", "intent": "unknown", "emotion": "neutral"}
        if self.current_speech_state:
            data_to_return = {
                "text": self.current_speech_state.text,
                "intent": self.current_speech_state.intent,
                "emotion": self.current_speech_state.emotion
            }
            if consume:
                print(f"ğŸ’¨ æ¶ˆè€—å·²å‘é€è¯­éŸ³: '{self.current_speech_state.text}'")
                self.current_speech_state = None
        return data_to_return
    
    def set_callback(self, callback: Callable[[MultimodalInput], None]):
        """è®¾ç½®å¤šæ¨¡æ€æ•°æ®å°±ç»ªå›è°ƒ"""
        self.on_multimodal_ready = callback
        print(f"âœ… å¤šæ¨¡æ€æ•°æ®å›è°ƒå·²è®¾ç½®: {callback.__qualname__ if hasattr(callback, '__qualname__') else str(callback)}")
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æ”¶é›†å™¨çŠ¶æ€"""
        with self._lock: # ç¡®ä¿çº¿ç¨‹å®‰å…¨åœ°è¯»å–çŠ¶æ€
            return {
                "gaze_threshold": self.gaze_threshold,
                "distraction_detected": self.distraction_detected,
                "distraction_start_time": self.distraction_start_time,
                "current_gaze": self._get_gaze_data(), # Use internal getters without consume flag
                "current_gesture": self._get_gesture_data(consume=False),
                "current_speech": self._get_speech_data(consume=False),
                "history_sizes": { # å†å²è®°å½•å¤§å°å¯èƒ½å¯¹è°ƒè¯•æœ‰ç”¨
                    "gaze": len(self.gaze_history),
                    "gesture": len(self.gesture_history),
                    "speech": len(self.speech_history)
                }
            }
    
    def reset(self):
        """é‡ç½®æ”¶é›†å™¨çŠ¶æ€"""
        with self._lock:
            self.current_gaze_state = None
            self.current_gesture_state = None
            self.current_speech_state = None
            self.distraction_detected = False
            self.distraction_start_time = None
            # æ¸…ç©ºå†å²è®°å½•æ˜¯å¯é€‰çš„ï¼Œä½†é€šå¸¸é‡ç½®æ„å‘³ç€ä»å¤´å¼€å§‹
            self.gaze_history.clear()
            self.gesture_history.clear()
            self.speech_history.clear()
            print("ğŸ”„ å¤šæ¨¡æ€æ”¶é›†å™¨å·²é‡ç½®")


# å…¨å±€å¤šæ¨¡æ€æ”¶é›†å™¨å®ä¾‹
multimodal_collector = MultimodalCollector() 
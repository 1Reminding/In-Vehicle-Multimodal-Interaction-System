import time
import math
import cv2
import dlib
import numpy as np
from imutils import face_utils
from scipy.spatial import distance as dist

# ---------------- å·¥å…·å‡½æ•° ----------------
def eye_aspect_ratio(eye):
    """çœ¨çœ¼æ¯”ä¾‹ EAR"""
    A = dist.euclidean(eye[1], eye[5])
    B = dist.euclidean(eye[2], eye[4])
    C = dist.euclidean(eye[0], eye[3])
    return (A + B) / (2.0 * C)

def mouth_aspect_ratio(mouth):
    """å¼ å˜´æ¯”ä¾‹ MAR"""
    A = np.linalg.norm(mouth[2] - mouth[9])
    B = np.linalg.norm(mouth[4] - mouth[7])
    C = np.linalg.norm(mouth[0] - mouth[6])
    return (A + B) / (2.0 * C)

def head_pose(shape, size):
    """
    solvePnP â†’ è¿”å› (yaw, pitch, roll) in degree.
    yaw>0: å·¦è½¬ï¼Œpitch>0: æŠ¬å¤´
    """

    model_pts = np.float32([
        (0.0,   0.0,   0.0),      # é¼»å°– 30
        (0.0,  -330.0, -65.0),    # ä¸‹å·´ 8
        (-225.0, 170.0, -135.0),  # å·¦çœ¼è§’ 36
        (225.0, 170.0, -135.0),   # å³çœ¼è§’ 45
        (-150.0, -150.0, -125.0), # å·¦å˜´è§’ 48
        (150.0, -150.0, -125.0)   # å³å˜´è§’ 54
    ])

    image_pts = np.float32([
        shape[30],  # é¼»å°–
        shape[8],   # ä¸‹å·´
        shape[36],  # å·¦çœ¼è§’
        shape[45],  # å³çœ¼è§’
        shape[48],  # å·¦å˜´è§’
        shape[54]   # å³å˜´è§’
    ])

    h, w = size
    focal = w
    center = (w / 2, h / 2)
    camera_mtx = np.array([[focal, 0, center[0]],
                           [0, focal, center[1]],
                           [0, 0, 1]], dtype=np.float32)
    dist_coef = np.zeros((4, 1))

    ok, rvec, tvec = cv2.solvePnP(model_pts, image_pts, camera_mtx,
                                  dist_coef, flags=cv2.SOLVEPNP_ITERATIVE)
    if not ok:
        return None

    rot_mat, _ = cv2.Rodrigues(rvec)
    pose_mat = cv2.hconcat((rot_mat, tvec))
    _, _, _, _, _, _, euler = cv2.decomposeProjectionMatrix(pose_mat)
    pitch, yaw, roll = [x[0] for x in euler]  # degree
    return yaw, pitch, roll


# ---------------- ä¸»æ£€æµ‹ ----------------
def live_detect():
    # ------------- å¸¸é‡ -------------
    CALIB_SECS   = 2               # åŸºçº¿é‡‡æ ·æ—¶é•¿
    FPS_GUESS    = 30
    MAR_THRESH   = 0.50            # å¼ å˜´é˜ˆå€¼ï¼ˆç»å¯¹æ¯”ä¾‹ï¼Œé€šå¸¸ç¨³å®šï¼‰
    YAW_THRESH   = 10              # Â°  |Yaw|>é˜ˆå€¼ â†’ å·¦/å³
    EAR_RATIO    = 0.90            # çœ¨çœ¼é˜ˆå€¼ = ear0 * 0.75
    EAR_FRAMES   = 2               # è¿ç»­å¸§
    PITCH_DELTA  = 6              # Â° ä½å¤´ç›¸å¯¹ä¸‹é™è§’
    PITCH_FRAMES = 1

    # ------------- è®¡æ•°å™¨ -------------
    ear_sum = pitch_sum = 0
    sample_cnt = 0
    calibrated = False
    ear0 = pitch0 = None

    c_eye = tot_eye = 0
    c_mouth = tot_mouth = 0
    yaw_dir = None
    yaw_flag = 0
    tot_shake = 0
    pitch_down_frames = 0
    tot_nod = 0
    prev_state = (-1, -1, -1, -1)
    done = False

    # ------------- æ¨¡å‹ -------------
    det = dlib.get_frontal_face_detector()
    pre = dlib.shape_predictor("shape_predictor_68_face_landmarks.dat")
    (lS, lE) = face_utils.FACIAL_LANDMARKS_IDXS["left_eye"]
    (rS, rE) = face_utils.FACIAL_LANDMARKS_IDXS["right_eye"]
    (mS, mE) = face_utils.FACIAL_LANDMARKS_IDXS["mouth"]

    # ------------- æ‘„åƒå¤´ -------------
    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return

    print("æ‘„åƒå¤´å·²æ‰“å¼€ï¼ŒæŒ‰ Q é€€å‡º...")

    while True:
        ret, frame = cap.read()
        if not ret:
            continue

        frame = cv2.resize(frame, (640, 480))
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        rects = det(gray, 0)
        if not rects:
            cv2.imshow("Live", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            continue

        shape = face_utils.shape_to_np(pre(gray, rects[0]))

        ear_val = (eye_aspect_ratio(shape[lS:lE]) +
                   eye_aspect_ratio(shape[rS:rE])) / 2.0
        mar_val = mouth_aspect_ratio(shape[mS:mE])
        pose = head_pose(shape, frame.shape[:2])

        # ---------- åŸºçº¿æ ¡å‡† ----------
        if not calibrated and pose:
            ear_sum += ear_val
            pitch_sum += pose[1]
            sample_cnt += 1
            if sample_cnt >= CALIB_SECS * FPS_GUESS:
                ear0   = ear_sum / sample_cnt
                pitch0 = pitch_sum / sample_cnt
                calibrated = True
                print(f"ğŸ“ åŸºçº¿å®Œæˆï¼šear0={ear0:.3f}, pitch0={pitch0:.1f}Â°")
            cv2.imshow("Live", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
            continue  # åœ¨æ ¡å‡†æœŸä¸åˆ¤å®šåŠ¨ä½œ

        # ---------------- çœ¨çœ¼ ----------------
        EAR_T = ear0 * EAR_RATIO
        c_eye = c_eye + 1 if ear_val < EAR_T else 0
        if c_eye >= EAR_FRAMES and ear_val >= EAR_T:
            tot_eye += 1

        # ---------------- å¼ å˜´ ----------------
        if mar_val > MAR_THRESH:
            c_mouth += 1
        elif c_mouth:
            tot_mouth += 1
            c_mouth = 0

        # ---------------- æ‘‡å¤´ ----------------
        yaw, pitch, _ = pose
        if abs(yaw) > YAW_THRESH:
            cur = 'L' if yaw > 0 else 'R'
            if yaw_dir != cur:
                yaw_flag += 1
                yaw_dir = cur
            if yaw_flag >= 2:
                tot_shake += 1
                yaw_flag = 0

        # ---------------- ç‚¹å¤´ ----------------
        if pitch < pitch0 - PITCH_DELTA:
            pitch_down_frames += 1
        else:
            if pitch_down_frames >= PITCH_FRAMES:
                tot_nod += 1
            pitch_down_frames = 0

        # ---------------- ç»ˆç«¯è¾“å‡º ----------------
        state = (tot_eye, tot_mouth, tot_shake, tot_nod)
        if state != prev_state:
            print(f"Blinks={tot_eye}  Mouth={tot_mouth}  "
                  f"Shake={tot_shake}  Nod={tot_nod}", flush=True)
            prev_state = state

        if tot_nod >= 2 and not done:
            done = True

        cv2.imshow("Live", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    live_detect()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡æ€èåˆäº¤äº’ç³»ç»Ÿä¸»ç¨‹åº

å®ç°äº†å®Œæ•´çš„å¤šæ¨¡æ€äº¤äº’æµç¨‹ï¼š
1. è§†è§‰æ¨¡æ€ï¼šçœ¼åŠ¨è¿½è¸ªã€æ‰‹åŠ¿è¯†åˆ«ã€å¤´éƒ¨å§¿æ€
2. è¯­éŸ³æ¨¡æ€ï¼šè¯­éŸ³è¯†åˆ«ã€æ„å›¾åˆ†ç±»
3. èåˆå†³ç­–ï¼šå¤šæ¨¡æ€æ•°æ®èåˆå’Œå†²çªè§£å†³
4. ç³»ç»Ÿå“åº”ï¼šæ–‡æœ¬ã€è¯­éŸ³ã€è§†è§‰åé¦ˆ

å…¸å‹åœºæ™¯ï¼šåˆ†å¿ƒæé†’
- è§¦å‘ï¼šè§†è§‰æ£€æµ‹åˆ°è§†çº¿åç¦»
- èåˆï¼šç­‰å¾…è¯­éŸ³ç¡®è®¤("å·²æ³¨æ„é“è·¯")æˆ–æ‰‹åŠ¿ç¡®è®¤(ç«–æ‹‡æŒ‡)
- å“åº”ï¼šå¤šæ¨¡æ€åé¦ˆç¡®è®¤ç”¨æˆ·æ³¨æ„åŠ›æ¢å¤
"""

import time
import cv2
import threading
import signal
import sys
from typing import Optional

# å¯¼å…¥ç°æœ‰æ¨¡å—
from modules.audio.recorder import Recorder
from modules.audio.speech_recognizer import transcribe
from modules.audio.intent_classifier import classify
from modules.vision.gesture_recognizer import GestureRecognizer
from modules.vision.head_pose_detector import HeadPoseDetector
from modules.vision.gaze_tracking import GazeTracking
from modules.vision.camera_manager import get_camera_manager, release_camera_manager

# å¯¼å…¥èåˆç³»ç»Ÿ
from modules.fusion import (
    initialize_fusion_system, get_system_status,
    event_bus, state_manager, fusion_engine, scenario_handler,
    ModalityEvent, EventType, ModalityType,
    AudioEvent, VisionEvent, GestureEvent
)


class MultimodalFusionApp:
    """å¤šæ¨¡æ€èåˆåº”ç”¨ç¨‹åº"""
    
    def __init__(self):
        self.running = False
        self.audio_thread = None
        self.vision_thread = None
        
        # åˆå§‹åŒ–èåˆç³»ç»Ÿ
        initialize_fusion_system()
        
        # è®¾ç½®ä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        print("ğŸš€ å¤šæ¨¡æ€èåˆç³»ç»Ÿå¯åŠ¨")
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        print(f"\nğŸ“¡ æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨å…³é—­ç³»ç»Ÿ...")
        self.stop()
    
    def start(self):
        """å¯åŠ¨ç³»ç»Ÿ"""
        self.running = True
        
        # å¯åŠ¨éŸ³é¢‘å¤„ç†çº¿ç¨‹
        self.audio_thread = threading.Thread(target=self._audio_worker, daemon=True)
        self.audio_thread.start()
        
        # å¯åŠ¨è§†è§‰å¤„ç†çº¿ç¨‹ï¼ˆä¸»çº¿ç¨‹ï¼‰
        self._vision_worker()
    
    def stop(self):
        """åœæ­¢ç³»ç»Ÿ"""
        self.running = False
        
        # é‡Šæ”¾èµ„æº
        release_camera_manager()
        
        # æ‰“å°ç³»ç»Ÿç»Ÿè®¡
        self._print_system_stats()
        
        print("ğŸ‘‹ ç³»ç»Ÿå·²å…³é—­")
        sys.exit(0)
    
    def _audio_worker(self):
        """éŸ³é¢‘å¤„ç†å·¥ä½œçº¿ç¨‹"""
        print("ğŸ¤ éŸ³é¢‘å¤„ç†çº¿ç¨‹å¯åŠ¨")
        
        try:
            rec = Recorder()
            for seg in rec.record_stream():
                if not self.running:
                    break
                
                # è¯­éŸ³è¯†åˆ«
                text = transcribe(seg["wav"])
                if text:
                    # åˆ›å»ºè¯­éŸ³è¯†åˆ«äº‹ä»¶
                    speech_event = AudioEvent(
                        event_type=EventType.SPEECH_RECOGNIZED,
                        data={"text": text, "audio_segment": seg},
                        confidence=0.8  # å¯ä»¥ä»è¯†åˆ«å™¨è·å–å®é™…ç½®ä¿¡åº¦
                    )
                    event_bus.publish(speech_event)
                    
                    # æ„å›¾åˆ†ç±»
                    intent_result = classify(text)
                    if intent_result:
                        # åˆ›å»ºæ„å›¾åˆ†ç±»äº‹ä»¶
                        intent_event = AudioEvent(
                            event_type=EventType.INTENT_CLASSIFIED,
                            data={
                                "text": text,
                                "intent": intent_result.get("intent"),
                                "confidence": intent_result.get("confidence", 0.5)
                            },
                            confidence=intent_result.get("confidence", 0.5)
                        )
                        event_bus.publish(intent_event)
                        
                        print(f"ğŸ¤ è¯­éŸ³: '{text}' -> {intent_result}")
                
        except Exception as e:
            print(f"âŒ éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
        
        print("ğŸ¤ éŸ³é¢‘å¤„ç†çº¿ç¨‹ç»“æŸ")
    
    def _vision_worker(self):
        """è§†è§‰å¤„ç†å·¥ä½œçº¿ç¨‹"""
        print("ğŸ‘ è§†è§‰å¤„ç†çº¿ç¨‹å¯åŠ¨")
        
        # è·å–æ‘„åƒå¤´ç®¡ç†å™¨
        camera_manager = get_camera_manager()
        
        # åˆå§‹åŒ–è§†è§‰æ¨¡å—
        gr = GestureRecognizer()
        hp = HeadPoseDetector()
        gaze = GazeTracking()
        
        if not camera_manager.is_opened:
            print("âŒ æ‘„åƒå¤´æ— æ³•æ‰“å¼€")
            return
        
        # è§†è§‰å¤„ç†çŠ¶æ€
        last_gesture = None
        last_gaze_state = None
        stable_gesture = None
        stable_start_time = None
        stability_threshold = 0.5
        
        try:
            while self.running:
                ok, frame = camera_manager.read_frame()
                if not ok:
                    time.sleep(0.01)
                    continue
                
                # æ£€æŸ¥åˆ†è¾¨ç‡å˜åŒ–
                current_width = camera_manager.width
                current_height = camera_manager.height
                if gr.image_width != current_width or gr.image_height != current_height:
                    gr.image_width = current_width
                    gr.image_height = current_height
                
                frame = cv2.flip(frame, 1)
                
                # çœ¼ç¥è¿½è¸ª
                gaze.refresh(frame)
                current_gaze_state = "center"
                if gaze.is_right():
                    current_gaze_state = "right"
                elif gaze.is_left():
                    current_gaze_state = "left"
                elif gaze.is_center():
                    current_gaze_state = "center"
                
                # åªåœ¨çŠ¶æ€å˜åŒ–æ—¶å‘å¸ƒäº‹ä»¶
                if current_gaze_state != last_gaze_state:
                    last_gaze_state = current_gaze_state
                    
                    gaze_event = VisionEvent(
                        event_type=EventType.GAZE_CHANGED,
                        data={
                            "state": current_gaze_state,
                            "timestamp": time.time()
                        },
                        confidence=0.9
                    )
                    event_bus.publish(gaze_event)
                
                # å¤´éƒ¨å§¿æ€æ£€æµ‹
                head_pose_result = hp.process_frame(frame)
                if head_pose_result:
                    if head_pose_result["type"] == "head_pose":
                        head_pose_event = VisionEvent(
                            event_type=EventType.HEAD_POSE_CHANGED,
                            data=head_pose_result,
                            confidence=0.8
                        )
                        event_bus.publish(head_pose_event)
                
                # æ‰‹åŠ¿è¯†åˆ«
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                rgb.flags.writeable = False
                res = gr.hands.process(rgb)
                rgb.flags.writeable = True
                
                current_time = time.time()
                current_gesture = None
                gesture_confidence = 0.0
                
                if res.multi_hand_landmarks:
                    for hlm in res.multi_hand_landmarks:
                        gesture, conf = gr._recognize_gesture(hlm)
                        if gesture:
                            current_gesture = gesture
                            gesture_confidence = conf
                            break
                
                # æ‰‹åŠ¿ç¨³å®šæ€§æ£€æµ‹
                if current_gesture == stable_gesture:
                    if stable_start_time is not None:
                        stable_duration = current_time - stable_start_time
                        if stable_duration >= stability_threshold and current_gesture != last_gesture:
                            last_gesture = current_gesture
                            
                            # å‘å¸ƒæ‰‹åŠ¿äº‹ä»¶
                            gesture_event = GestureEvent(
                                event_type=EventType.GESTURE_DETECTED,
                                data={
                                    "gesture": current_gesture,
                                    "confidence": gesture_confidence,
                                    "stable_duration": stable_duration,
                                    "timestamp": current_time
                                },
                                confidence=gesture_confidence
                            )
                            event_bus.publish(gesture_event)
                else:
                    stable_gesture = current_gesture
                    stable_start_time = current_time if current_gesture else None
                
                # æ˜¾ç¤ºå½“å‰çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
                self._display_status_overlay(frame)
                
                # æ˜¾ç¤ºè§†é¢‘ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
                if False:  # è®¾ç½®ä¸ºTrueä»¥æ˜¾ç¤ºè§†é¢‘çª—å£
                    cv2.imshow('Multimodal Fusion System', frame)
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break
                
                time.sleep(0.01)  # æ§åˆ¶å¸§ç‡
                
        except Exception as e:
            print(f"âŒ è§†è§‰å¤„ç†é”™è¯¯: {e}")
        finally:
            gr.hands.close()
            cv2.destroyAllWindows()
        
        print("ğŸ‘ è§†è§‰å¤„ç†çº¿ç¨‹ç»“æŸ")
    
    def _display_status_overlay(self, frame):
        """åœ¨è§†é¢‘å¸§ä¸Šæ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯"""
        # è·å–ç³»ç»ŸçŠ¶æ€
        status = get_system_status()
        current_state = status["current_state"]
        active_session = status["active_session"]
        
        # åœ¨å¸§ä¸Šç»˜åˆ¶çŠ¶æ€ä¿¡æ¯
        y_offset = 30
        cv2.putText(frame, f"State: {current_state}", (10, y_offset), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        if active_session:
            y_offset += 30
            scenario = active_session["scenario"]
            duration = active_session["duration"]
            cv2.putText(frame, f"Session: {scenario} ({duration:.1f}s)", 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
            
            y_offset += 25
            expected = ", ".join(active_session["expected_modalities"])
            cv2.putText(frame, f"Expected: {expected}", 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    def _print_system_stats(self):
        """æ‰“å°ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯:")
        status = get_system_status()
        
        print(f"   å½“å‰çŠ¶æ€: {status['current_state']}")
        
        session_stats = status['session_stats']
        if session_stats['total_sessions'] > 0:
            print(f"   ä¼šè¯ç»Ÿè®¡: {session_stats['total_sessions']} æ€»æ•°, "
                  f"{session_stats['completed_sessions']} å®Œæˆ, "
                  f"æˆåŠŸç‡ {session_stats['completion_rate']:.1%}")
        
        fusion_stats = status['fusion_stats']
        if fusion_stats['total_fusions'] > 0:
            print(f"   èåˆç»Ÿè®¡: {fusion_stats['total_fusions']} æ€»æ•°, "
                  f"æˆåŠŸç‡ {fusion_stats['success_rate']:.1%}")
        
        response_stats = status['response_stats']
        if response_stats['total_responses'] > 0:
            print(f"   å“åº”ç»Ÿè®¡: {response_stats['total_responses']} æ€»æ•°")
        
        print(f"   äº‹ä»¶å†å²: {status['event_history_size']} æ¡è®°å½•")
    
    def demo_distraction_scenario(self):
        """æ¼”ç¤ºåˆ†å¿ƒæé†’åœºæ™¯"""
        print("\nğŸ­ æ¼”ç¤ºåˆ†å¿ƒæé†’åœºæ™¯")
        
        # æ¨¡æ‹Ÿè§†çº¿åç¦»äº‹ä»¶
        gaze_event = VisionEvent(
            event_type=EventType.GAZE_CHANGED,
            data={"state": "left", "timestamp": time.time()},
            confidence=0.9
        )
        event_bus.publish(gaze_event)
        
        print("   1. æ¨¡æ‹Ÿè§†çº¿åç¦» -> è§¦å‘åˆ†å¿ƒæ£€æµ‹")
        time.sleep(2)
        
        # æ¨¡æ‹Ÿè¯­éŸ³ç¡®è®¤
        speech_event = AudioEvent(
            event_type=EventType.SPEECH_RECOGNIZED,
            data={"text": "å·²æ³¨æ„é“è·¯"},
            confidence=0.8
        )
        event_bus.publish(speech_event)
        
        intent_event = AudioEvent(
            event_type=EventType.INTENT_CLASSIFIED,
            data={"text": "å·²æ³¨æ„é“è·¯", "intent": "attention_confirm"},
            confidence=0.9
        )
        event_bus.publish(intent_event)
        
        print("   2. æ¨¡æ‹Ÿè¯­éŸ³ç¡®è®¤ -> èåˆå†³ç­–")
        time.sleep(1)
        
        # æ¨¡æ‹Ÿæ‰‹åŠ¿ç¡®è®¤
        gesture_event = GestureEvent(
            event_type=EventType.GESTURE_DETECTED,
            data={"gesture": "thumbs_up", "confidence": 0.8},
            confidence=0.8
        )
        event_bus.publish(gesture_event)
        
        print("   3. æ¨¡æ‹Ÿæ‰‹åŠ¿ç¡®è®¤ -> å®Œæˆäº¤äº’")
        time.sleep(2)
        
        print("âœ… æ¼”ç¤ºå®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    app = MultimodalFusionApp()
    
    # å¯é€‰ï¼šè¿è¡Œæ¼”ç¤º
    if len(sys.argv) > 1 and sys.argv[1] == "--demo":
        print("ğŸ­ è¿è¡Œæ¼”ç¤ºæ¨¡å¼")
        threading.Timer(3.0, app.demo_distraction_scenario).start()
    
    try:
        app.start()
    except KeyboardInterrupt:
        print("\nâŒ¨ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")
    finally:
        app.stop()


if __name__ == "__main__":
    main() 
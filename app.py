#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è½¦è½½å¤šæ¨¡æ€æ™ºèƒ½äº¤äº’ç³»ç»Ÿ - AIå¢å¼ºç‰ˆ

é›†æˆDeepSeek APIè¿›è¡Œæ™ºèƒ½å¤šæ¨¡æ€èåˆå’Œè¯­éŸ³åé¦ˆ
"""

import time
import cv2
import threading
import signal
import sys
import json
from typing import Dict, Any

# å¯¼å…¥ç°æœ‰æ¨¡å—
from modules.audio.recorder import Recorder
from modules.audio.speech_recognizer import transcribe
from modules.vision.gesture.gesture_recognizer import GestureRecognizer
from modules.vision.head.head_pose_detector import HeadPoseDetector
from modules.vision.gaze.gaze_tracking import GazeTracking
from modules.vision.camera_manager import get_camera_manager, release_camera_manager
from modules.actions.action_handler import handle_action


# å¯¼å…¥AIæ¨¡å—
from modules.ai.deepseek_client import deepseek_client, MultimodalInput, AIResponse
from modules.ai.multimodal_collector import multimodal_collector

import os
#from PySide6.QtGui import QGuiApplication
#from PySide6.QtQml import QQmlApplicationEngine
#from PySide6.QtCore import QUrl, QObject, Signal, Slot

import requests
from PyQt5.QtGui     import QGuiApplication
from PyQt5.QtQml     import QQmlApplicationEngine
from PyQt5.QtCore   import QUrl, QObject, pyqtSignal, pyqtSlot, QTimer


class UIBackend(QObject):
    """æš´éœ²ç»™ QML çš„æ¡¥æ¥å¯¹è±¡"""
    commandIssued = pyqtSignal(str)

    weatherUpdated = pyqtSignal(str)

    @pyqtSlot(str)
    def requestAction(self, cmd):
        print(f"ğŸ”· å‰ç«¯è¯·æ±‚åŠ¨ä½œï¼š{cmd}")
        handle_action(cmd)

# å®ä¾‹åŒ–
ui_backend = UIBackend()

class AIMultimodalApp:
    """AIå¢å¼ºçš„å¤šæ¨¡æ€äº¤äº’åº”ç”¨"""
    
    def __init__(self):
        self.running = False
        self.audio_thread = None
        self.vision_thread = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "ai_requests": 0,
            "successful_responses": 0,
            "speech_inputs": 0,
            "gesture_detections": 0,
            "gaze_changes": 0,
            "start_time": time.time()
        }
        
        # è®¾ç½®å¤šæ¨¡æ€æ•°æ®å›è°ƒ
        multimodal_collector.set_callback(self.on_multimodal_data_ready)
        
        print("ğŸš€ AIå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print("ğŸ“‹ åŠŸèƒ½è¯´æ˜:")
        print("   - çœ¼åŠ¨åç¦»è¶…è¿‡3ç§’è§¦å‘AIåˆ†æ")
        print("   - è¯­éŸ³è¾“å…¥ç«‹å³è§¦å‘AIåˆ†æ")
        print("   - æ‰‹åŠ¿è¯†åˆ«è§¦å‘AIåˆ†æ")
        print("   - AIåˆ†æç»“æœé€šè¿‡æ–‡æœ¬æ˜¾ç¤º")
        print("   - æŒ‰ Ctrl+C é€€å‡ºç³»ç»Ÿ")
    
    def on_multimodal_data_ready(self, multimodal_input: MultimodalInput):
        """å¤„ç†å¤šæ¨¡æ€æ•°æ®å°±ç»ªäº‹ä»¶"""
        print(f"\nğŸ¤– AIåˆ†æå¼€å§‹...")
        print(f"ğŸ“Š è¾“å…¥æ•°æ®:")
        print(f"   ğŸ‘ çœ¼åŠ¨: {multimodal_input.gaze_data['state']} ({multimodal_input.gaze_data['duration']:.1f}s)")
        print(f"   ğŸ– æ‰‹åŠ¿: {multimodal_input.gesture_data['gesture']} ({multimodal_input.gesture_data['confidence']:.2f})")
        print(f"   ğŸ¤ è¯­éŸ³: '{multimodal_input.speech_data['text']}'")
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats["ai_requests"] += 1
        
        try:
            # è°ƒç”¨DeepSeek APIè¿›è¡Œåˆ†æ
            ai_response = deepseek_client.analyze_multimodal_data(multimodal_input)
            
            print(f"\nğŸ§  AIåˆ†æç»“æœ:")
            print(f"   ğŸ“‹ æ¨èæ“ä½œ: {ai_response.recommendation_text}")
            print(f"   ğŸ¯ ç½®ä¿¡åº¦: {ai_response.confidence:.2f}")
            print(f"   ğŸ’­ æ¨ç†è¿‡ç¨‹: {ai_response.reasoning}")
            
            # è§£ææ“ä½œæŒ‡ä»¤
            try:
                action_data = json.loads(ai_response.action_code)
                print(f"   âš™ï¸ æ“ä½œæŒ‡ä»¤: {action_data}")
                handle_action(action_data)
                ui_backend.commandIssued.emit(action_data)

            except json.JSONDecodeError:
                print(f"   âš™ï¸ æ“ä½œæŒ‡ä»¤: {ai_response.action_code}")       
                handle_action(ai_response.action_code)
                ui_backend.commandIssued.emit(ai_response.action_code)
       
            
            # æ–‡æœ¬åé¦ˆï¼ˆä¸ä½¿ç”¨TTSï¼‰
            if ai_response.recommendation_text:
                print(f"ğŸ’¬ ç³»ç»Ÿå»ºè®®: {ai_response.recommendation_text}")
            
            # æ·»åŠ åˆ°å¯¹è¯å†å²
            deepseek_client.add_to_conversation_history(multimodal_input, ai_response)
            
            self.stats["successful_responses"] += 1
            
        except Exception as e:
            print(f"âŒ AIåˆ†æå¤±è´¥: {e}")
            print("ğŸ’¬ ç³»ç»Ÿæç¤º: æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚")
    
    def audio_worker(self):
        """éŸ³é¢‘å·¥ä½œçº¿ç¨‹"""
        print("ğŸ¤ éŸ³é¢‘çº¿ç¨‹å¯åŠ¨")
        rec = Recorder()
        
        try:
            for seg in rec.record_stream():
                if not self.running:
                    break
                
                # è¯­éŸ³è¯†åˆ«
                text = transcribe(seg["wav"])
                if not text or not text.strip():
                    continue
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats["speech_inputs"] += 1
                
                print(f"ğŸ¤ è¯­éŸ³è¯†åˆ«: '{text}'")
                
                # æ›´æ–°å¤šæ¨¡æ€æ”¶é›†å™¨
                speech_data = {
                    "text": text,
                }
                multimodal_collector.update_speech_data(speech_data)
                
        except Exception as e:
            print(f"âŒ éŸ³é¢‘çº¿ç¨‹é”™è¯¯: {e}")
        finally:
            print("ğŸ¤ éŸ³é¢‘çº¿ç¨‹ç»“æŸ")
    
    def vision_worker(self):
        """è§†è§‰å·¥ä½œçº¿ç¨‹"""
        print("ğŸ‘ è§†è§‰çº¿ç¨‹å¯åŠ¨")
        
        # è·å–æ‘„åƒå¤´ç®¡ç†å™¨
        camera_manager = get_camera_manager()
        
        # åˆå§‹åŒ–è§†è§‰æ¨¡å—
        gr = GestureRecognizer()
        hp = HeadPoseDetector()
        gaze = GazeTracking()
        
        if not camera_manager.is_opened:
            print("âŒ æ‘„åƒå¤´æ— æ³•æ‰“å¼€")
            return
        
        # çŠ¶æ€å˜é‡
        last_gesture = None
        last_gaze_state = None
        stable_gesture = None
        stable_start_time = None
        stability_threshold = 0.5
        
        try:
            while self.running:
                ok, frame = camera_manager.read_frame()
                if not ok:
                    time.sleep(0.01)
                    continue
                
                frame = cv2.flip(frame, 1)
                
                # çœ¼åŠ¨è¿½è¸ª
                gaze.refresh(frame)
                current_gaze_state = "center"
                if gaze.is_right():
                    current_gaze_state = "right"
                elif gaze.is_left():
                    current_gaze_state = "left"
                elif gaze.is_center():
                    current_gaze_state = "center"
                
                # çœ¼åŠ¨çŠ¶æ€å˜åŒ–æ—¶æ›´æ–°æ”¶é›†å™¨
                if current_gaze_state != last_gaze_state:
                    last_gaze_state = current_gaze_state
                    self.stats["gaze_changes"] += 1
                    
                    gaze_data = {
                        "state": current_gaze_state,
                        "ts": time.time()
                    }
                    multimodal_collector.update_gaze_data(gaze_data)
                
                # å¤´éƒ¨å§¿æ€æ£€æµ‹
                head_pose_result = hp.process_frame(frame)
                if head_pose_result:
                    if head_pose_result["type"] == "head_pose_calibrated":
                        print(f"ğŸ¯ å¤´éƒ¨å§¿æ€åŸºçº¿æ ¡å‡†: pitch0={head_pose_result['pitch0']:.1f}Â°")
                    elif head_pose_result["type"] == "head_pose":
                        print(f"ğŸ—£ å¤´éƒ¨å§¿æ€: {head_pose_result}")
                
                # æ‰‹åŠ¿è¯†åˆ«
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                rgb.flags.writeable = False
                res = gr.hands.process(rgb)
                rgb.flags.writeable = True
                
                current_time = time.time()
                current_gesture = None
                current_conf = 0.0
                
                if res.multi_hand_landmarks:
                    for hlm in res.multi_hand_landmarks:
                        gesture, conf = gr._recognize_gesture(hlm)
                        if gesture:
                            current_gesture = gesture
                            current_conf = conf
                            break
                
                # æ‰‹åŠ¿ç¨³å®šæ€§æ£€æµ‹
                if current_gesture == stable_gesture:
                    if stable_start_time is not None:
                        stable_duration = current_time - stable_start_time
                        if stable_duration >= stability_threshold and current_gesture != last_gesture:
                            last_gesture = current_gesture
                            self.stats["gesture_detections"] += 1
                            
                            gesture_data = {
                                "gesture": current_gesture,
                                "conf": current_conf,
                                "ts": current_time,
                                "stable_duration": stable_duration
                            }
                            
                            print(f"ğŸ– æ‰‹åŠ¿æ£€æµ‹: {current_gesture} (ç½®ä¿¡åº¦: {current_conf:.2f})")
                            multimodal_collector.update_gesture_data(gesture_data)
                else:
                    stable_gesture = current_gesture
                    stable_start_time = current_time if current_gesture else None
                
        except Exception as e:
            print(f"âŒ è§†è§‰çº¿ç¨‹é”™è¯¯: {e}")
        finally:
            gr.hands.close()
            print("ğŸ‘ è§†è§‰çº¿ç¨‹ç»“æŸ")
    
    def print_status(self):
        """æ‰“å°ç³»ç»ŸçŠ¶æ€"""
        runtime = time.time() - self.stats["start_time"]
        
        print(f"\nğŸ“Š ç³»ç»ŸçŠ¶æ€ (è¿è¡Œæ—¶é—´: {runtime:.1f}ç§’)")
        print(f"   ğŸ¤– AIè¯·æ±‚æ¬¡æ•°: {self.stats['ai_requests']}")
        print(f"   âœ… æˆåŠŸå“åº”: {self.stats['successful_responses']}")
        print(f"   ğŸ¤ è¯­éŸ³è¾“å…¥: {self.stats['speech_inputs']}")
        print(f"   ğŸ– æ‰‹åŠ¿æ£€æµ‹: {self.stats['gesture_detections']}")
        print(f"   ğŸ‘ çœ¼åŠ¨å˜åŒ–: {self.stats['gaze_changes']}")
        
        # å¤šæ¨¡æ€æ”¶é›†å™¨çŠ¶æ€
        collector_status = multimodal_collector.get_status()
        print(f"   ğŸ“‹ æ”¶é›†å™¨çŠ¶æ€: {'æ”¶é›†ä¸­' if collector_status['is_collecting'] else 'å¾…æœº'}")
    
    def signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        print(f"\nğŸ›‘ æ¥æ”¶åˆ°é€€å‡ºä¿¡å· ({signum})")
        self.stop()
    
    def start(self):
        """å¯åŠ¨åº”ç”¨"""
        print("ğŸš€ å¯åŠ¨AIå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿ...")
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨
        #signal.signal(signal.SIGINT, self.signal_handler)
        #signal.signal(signal.SIGTERM, self.signal_handler)
        
        self.running = True
        
        # å¯åŠ¨éŸ³é¢‘çº¿ç¨‹
        self.audio_thread = threading.Thread(target=self.audio_worker, daemon=True)
        self.audio_thread.start()
        
        # å¯åŠ¨è§†è§‰çº¿ç¨‹ï¼ˆä¸»çº¿ç¨‹ï¼‰
        try:
            self.vision_worker()
        except KeyboardInterrupt:
            print("\nâŒ¨ï¸ ç”¨æˆ·ä¸­æ–­")
        finally:
            self.stop()
    
    def stop(self):
        """åœæ­¢åº”ç”¨"""
        if not self.running:
            return
        
        print("ğŸ›‘ æ­£åœ¨åœæ­¢AIå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿ...")
        self.running = False
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        if self.audio_thread and self.audio_thread.is_alive():
            self.audio_thread.join(timeout=2.0)
        
        # æ‰“å°æœ€ç»ˆçŠ¶æ€
        self.print_status()
        
        # å…³é—­èµ„æº
        release_camera_manager()
        
        print("âœ… AIå¤šæ¨¡æ€äº¤äº’ç³»ç»Ÿå·²åœæ­¢")


def fetch_weather(city="Tianjin"):
    #api_key = "e8527d822a260a90258bbbcf110506e8"
    url = f"http://api.openweathermap.org/data/2.5/weather?q=Tianjin&appid=e8527d822a260a90258bbbcf110506e8&units=metric&lang=zh_cn"

    try:
        res = requests.get(url, timeout=5)
        data = res.json()
        if res.status_code == 200:
            temp = data['main']['temp']
            desc = data['weather'][0]['description']
            return f"{round(temp)}Â°C {desc}"
        else:
            print("âŒ å¤©æ°” API å“åº”å¤±è´¥ï¼š", data)
            return "å¤©æ°”åŠ è½½å¤±è´¥"
    except Exception as e:
        print("âŒ è¯·æ±‚å¤©æ°”å¤±è´¥ï¼š", e)
        return "å¤©æ°”å¼‚å¸¸"

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš— è½¦è½½å¤šæ¨¡æ€æ™ºèƒ½äº¤äº’ç³»ç»Ÿ - AIå¢å¼ºç‰ˆ")
    print("=" * 60)
    
    # 1. å¯åŠ¨åç«¯å¤šæ¨¡æ€æœåŠ¡ï¼ˆåœ¨åå°çº¿ç¨‹ï¼‰
    backend = AIMultimodalApp()
    
    signal.signal(signal.SIGINT, backend.signal_handler)
    signal.signal(signal.SIGTERM, backend.signal_handler)

    threading.Thread(target=backend.start, daemon=True).start()


    # 2. å®ä¾‹åŒ– UIBackend
    #ui_backend = UIBackend()

    # 3. å¯åŠ¨ QML ç•Œé¢
    app = QGuiApplication(sys.argv)
    engine = QQmlApplicationEngine()
    engine.rootContext().setContextProperty("UIBackend", ui_backend)

    qml_path = os.path.join(os.path.dirname(__file__), "ui", "Main.qml")
    engine.load(QUrl.fromLocalFile(qml_path))
    if not engine.rootObjects():
        print("âŒ æ— æ³•åŠ è½½ QML ç•Œé¢ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–è¯­æ³•")
        return 1

    weather_text = fetch_weather("Tianjin")
    QTimer.singleShot(10, lambda: ui_backend.weatherUpdated.emit(weather_text))

    # 4. è¿›å…¥ Qt äº‹ä»¶å¾ªç¯ï¼ˆé˜»å¡ï¼‰
    return app.exec_()


if __name__ == "__main__":
    main() 
# ä»…æ¼”ç¤ºï¼šè¯­éŸ³çº¿ç¨‹ + æ‰‹åŠ¿çº¿ç¨‹ + å¤´éƒ¨å§¿æ€çº¿ç¨‹ â†’ æ‰“å°è¾“å‡º
import time
import cv2
import threading
from modules.audio.recorder import Recorder
from modules.audio.speech_recognizer import transcribe
from modules.audio.intent_classifier import classify
from modules.vision.gesture_recognizer import GestureRecognizer
from modules.vision.head_pose_detector import HeadPoseDetector
from modules.vision.gaze_tracking import GazeTracking


def audio_worker():
    rec = Recorder()
    for seg in rec.record_stream():
        text = transcribe(seg["wav"])
        res = classify(text)
        if res:
            print("ğŸ¤", res)


def vision_worker():
    gr = GestureRecognizer()
    hp = HeadPoseDetector()  # åˆå§‹åŒ–å¤´éƒ¨å§¿æ€æ£€æµ‹å™¨
    gaze = GazeTracking()    # åˆå§‹åŒ–çœ¼ç¥è¿½è¸ª
    
    if not gr.cap.isOpened():
        raise RuntimeError("æ‘„åƒå¤´æ— æ³•æ‰“å¼€")

    last_gesture = None
    last_conf = 0.0
    # æ·»åŠ ç¨³å®šæ€§æ£€æµ‹å˜é‡
    stable_gesture = None
    stable_start_time = None
    stability_threshold = 0.5  # åŠç§’ç¨³å®šæ—¶é—´
    
    # çœ¼ç¥è¿½è¸ªçŠ¶æ€å˜é‡
    last_gaze_state = None

    try:
        while True:
            ok, frame = gr.cap.read()
            if not ok:
                time.sleep(0.01)
                continue

            # æ£€æŸ¥åˆ†è¾¨ç‡æ˜¯å¦å˜åŠ¨
            current_width = int(gr.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            current_height = int(gr.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            if gr.image_width != current_width or gr.image_height != current_height:
                gr.image_width = current_width
                gr.image_height = current_height

            frame = cv2.flip(frame, 1)
            
            # çœ¼ç¥è¿½è¸ªæ£€æµ‹
            gaze.refresh(frame)
            current_gaze_state = "center"
            if gaze.is_right():
                current_gaze_state = "right"
            elif gaze.is_left():
                current_gaze_state = "left"
            elif gaze.is_center():
                current_gaze_state = "center"
                
            # åªåœ¨çŠ¶æ€å˜åŒ–æ—¶è¾“å‡º
            if current_gaze_state != last_gaze_state:
                last_gaze_state = current_gaze_state
                print("ğŸ‘", {
                    "type": "gaze",
                    "state": current_gaze_state,
                    "ts": time.time()
                })
            
            # å¤´éƒ¨å§¿æ€æ£€æµ‹
            head_pose_result = hp.process_frame(frame)
            if head_pose_result:
                if head_pose_result["type"] == "head_pose_calibrated":
                    print("ğŸ¯", f"å¤´éƒ¨å§¿æ€åŸºçº¿æ ¡å‡†å®Œæˆ: pitch0={head_pose_result['pitch0']:.1f}Â°")
                elif head_pose_result["type"] == "head_pose":
                    print("ğŸ—£", head_pose_result)
            
            # æ‰‹åŠ¿æ£€æµ‹
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            rgb.flags.writeable = False
            res = gr.hands.process(rgb)
            rgb.flags.writeable = True

            current_time = time.time()
            current_gesture = None

            if res.multi_hand_landmarks:
                for hlm in res.multi_hand_landmarks:
                    gesture, conf = gr._recognize_gesture(hlm)
                    if gesture:
                        current_gesture = gesture
                        break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„æ‰‹åŠ¿

            # ç¨³å®šæ€§æ£€æµ‹é€»è¾‘
            if current_gesture == stable_gesture:
                # æ‰‹åŠ¿ä¿æŒä¸å˜
                if stable_start_time is not None:
                    stable_duration = current_time - stable_start_time
                    if stable_duration >= stability_threshold and current_gesture != last_gesture:
                        # æ‰‹åŠ¿ç¨³å®šè¶…è¿‡é˜ˆå€¼ä¸”ä¸ä¸Šæ¬¡è¾“å‡ºä¸åŒ
                        last_gesture = current_gesture
                        print("ğŸ–", {
                            "type": "gesture",
                            "gesture": current_gesture,
                            "conf": float(conf) if current_gesture else 0.0,
                            "ts": current_time,
                            "stable_duration": stable_duration
                        })
            else:
                # æ‰‹åŠ¿å‘ç”Ÿå˜åŒ–ï¼Œé‡ç½®ç¨³å®šæ€§è®¡æ—¶
                stable_gesture = current_gesture
                stable_start_time = current_time if current_gesture else None

    finally:
        gr.cap.release()
        gr.hands.close()


if __name__ == "__main__":
    threading.Thread(target=audio_worker, daemon=True).start()
    vision_worker()  # ä¸»çº¿ç¨‹è·‘æ‘„åƒå¤´ï¼ŒCtrlâ€‘C é€€å‡º
